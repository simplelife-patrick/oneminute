//
//  DLYAVEngine.m
//  OneMinute
//
//  Created by chenzonghai on 19/07/2017.
//  Copyright Â© 2017 åŠ¨æ—…æ¸¸. All rights reserved.
//

#import "DLYAVEngine.h"
#import "DLYMobileDevice.h"
#import "DLYResource.h"
#import <GPUImageMovie.h>
#import <GPUImageMovieWriter.h>
#import <GPUImageChromaKeyBlendFilter.h>
#import <AssetsLibrary/AssetsLibrary.h>
#import "DLYResource.h"
#import "DLYTransitionComposition.h"
#import "DLYTransitionInstructions.h"
#import "DLYVideoTransition.h"
#import "DLYResource.h"
#import "DLYSession.h"
#import "ALAssetsLibrary+CustomPhotoAlbum.h"
#import <math.h>
#import "DLYMovieObject.h"
#import <CoreMotion/CoreMotion.h>
#import "DLYThemesData.h"
#import "DLYVideoFilter.h"

typedef void (^OnBufferBlock)(CMSampleBufferRef sampleBuffer);

@interface DLYAVEngine ()<AVCaptureVideoDataOutputSampleBufferDelegate,AVCaptureAudioDataOutputSampleBufferDelegate,AVCaptureMetadataOutputObjectsDelegate,CAAnimationDelegate>
{
    AVCaptureVideoOrientation _videoOrientation;
    dispatch_queue_t _movieWritingQueue;
    CMBufferQueueRef _previewBufferQueue;
    BOOL _recordingWillBeStarted;
    BOOL _readyToRecordAudio;
    BOOL _readyToRecordVideo;
    
    CMTime _startTime;
    CMTime _stopTime;
    CMTime _prePoint;
    CGSize _videoSize;
    NSURL *_fileUrl;
    CGRect _faceRegion;
    CGRect _lastFaceRegion;
    BOOL isDetectedMetadataObjectTarget;
    BOOL isMicGranted;//éº¦å…‹é£æƒé™æ˜¯å¦è¢«å…è®¸
    
    int _channels;//éŸ³é¢‘é€šé“
    Float64 _samplerate;//éŸ³é¢‘é‡‡æ ·ç‡
    AVAssetExportSession *_exportSession;
    CMTime _timeOffset;//å½•åˆ¶çš„åç§»CMTime
    CMTime _lastVideo;//è®°å½•ä¸Šä¸€æ¬¡è§†é¢‘æ•°æ®æ–‡ä»¶çš„CMTime
    CMTime _lastAudio;//è®°å½•ä¸Šä¸€æ¬¡éŸ³é¢‘æ•°æ®æ–‡ä»¶çš„CMTime
    CocoaSecurityResult *_result;
    BOOL _isRecordingCancel;
    
    AVCaptureVideoOrientation _referenceOrientation;
}

@property (nonatomic, strong) AVCaptureMetadataOutput           *metadataOutput;
@property (nonatomic, strong) AVCaptureDeviceInput              *frontCameraInput;
@property (nonatomic, strong) AVCaptureDeviceInput              *audioMicInput;
@property (nonatomic, strong) AVCaptureDeviceFormat             *defaultFormat;
@property (nonatomic, strong) AVCaptureConnection               *audioConnection;

@property (nonatomic, strong) AVAssetWriter                     *assetWriter;
@property (nonatomic, strong) AVAssetWriterInput                *assetWriterVideoInput;
@property (nonatomic, strong) AVAssetWriterInput                *assetWriterAudioInput;

@property (nonatomic, strong) AVCaptureVideoDataOutput          *videoDataOutput;
@property (nonatomic, strong) AVCaptureAudioDataOutput          *audioDataOutput;
@property (nonatomic, strong) AVCaptureDeviceInput              *currentVideoDeviceInput;

@property (nonatomic, strong) GPUImageMovie                     *alphaMovie;
@property (nonatomic, strong) GPUImageMovie                     *bodyMovie;
@property (nonatomic, strong) GPUImageMovieWriter               *movieWriter;
@property (nonatomic, strong) GPUImageChromaKeyBlendFilter      *filter;
typedef void ((^MixcompletionBlock) (NSURL *outputUrl));

@property (nonatomic, strong) AVMutableComposition              *composition;
@property (nonatomic, strong) NSMutableArray                    *passThroughTimeRanges;
@property (nonatomic, strong) NSMutableArray                    *transitionTimeRanges;
@property (nonatomic, strong) UIImagePickerController           *moviePicker;

@property (nonatomic, strong) DLYResource                       *resource;
@property (nonatomic, strong) DLYSession                        *session;

@property (nonatomic, strong) AVMutableVideoComposition         *videoComposition;
@property (nonatomic, strong) AVAssetExportSession              *assetExporter;

@property (atomic, assign) BOOL isCapturing;//æ­£åœ¨å½•åˆ¶
@property (atomic, assign) BOOL isPaused;//æ˜¯å¦æš‚åœ
@property (nonatomic, strong) NSMutableArray *imageArr;
@property (nonatomic, strong) NSTimer *recordTimer; //å‡†å¤‡æ‹æ‘„ç‰‡æ®µé—ªçƒçš„è®¡æ—¶å™¨

@property (nonatomic) CMTime                                   defaultMinFrameDuration;
@property (nonatomic) CMTime                                   defaultMaxFrameDuration;
@property (nonatomic, strong) NSString                         *plistPath;
@property (nonatomic, strong) NSString                         *currentDeviceType;

@property (nonatomic, copy) OnBufferBlock                      onBuffer;

@property (retain, nonatomic) GPUImageMovie *movieFile;
@property (retain, nonatomic) GPUImageOutput<GPUImageInput> *outputFilter;
@property (retain, nonatomic) GPUImageMovieWriter *inputMovieWriter;

@end

@implementation DLYAVEngine

- (BOOL)shouldAutorotate
{
    return NO;
}

- (NSUInteger)supportedInterfaceOrientations
{
    return UIInterfaceOrientationMaskAll;
}

+ (instancetype) sharedDLYAVEngine{
    
    static DLYAVEngine *AVEngine;
    static dispatch_once_t onceToken;
    dispatch_once(&onceToken, ^{
        AVEngine = [[DLYAVEngine alloc] init];
    });
    return AVEngine;
}
- (void)dealloc {
    
    [_captureSession stopRunning];
    _captureSession             = nil;
    _captureVideoPreviewLayer   = nil;
    
    _backCameraInput            = nil;
    _frontCameraInput           = nil;
    
    _audioDataOutput            = nil;
    _videoDataOutput            = nil;
    
    _audioConnection            = nil;
    _videoConnection            = nil;
}
#pragma mark - Lazy Load -

-(DLYResource *)resource{
    if (!_resource) {
        _resource = [[DLYResource alloc] init];
    }
    return _resource;
}

-(NSMutableArray *)imageArray{
    if (_imageArray) {
        _imageArray = [NSMutableArray array];
    }
    return _imageArray;
}
-(DLYMiniVlogPart *)currentPart{
    if (!_currentPart) {
        _currentPart = [[DLYMiniVlogPart alloc] init];
    }
    return _currentPart;
}
-(DLYSession *)session{
    if (!_session) {
        _session = [[DLYSession alloc] init];
    }
    return _session;
}
#pragma mark - åˆ›å»ºRecorderå½•åˆ¶ä¼šè¯ -
-(AVCaptureSession *)captureSession{
    if (_captureSession == nil) {
        _captureSession = [[AVCaptureSession alloc] init];
        _captureSession.sessionPreset = AVCaptureSessionPreset1280x720;
        
        //æ·»åŠ åç½®æ‘„åƒå¤´çš„è¾“å…¥
        if ([self.captureSession canAddInput:self.backCameraInput]) {
            [self.captureSession addInput:self.backCameraInput];
        }
        //æ·»åŠ éº¦å…‹é£çš„è¾“å…¥
        if ([_captureSession canAddInput:self.audioMicInput]) {
            [_captureSession addInput:self.audioMicInput];
        }
        //æ·»åŠ è§†é¢‘è¾“å‡º
        if ([_captureSession canAddOutput:self.videoDataOutput]) {
            [_captureSession addOutput:self.videoDataOutput];
        }
        //æ·»åŠ éŸ³é¢‘è¾“å‡º
        if ([_captureSession canAddOutput:self.audioDataOutput]) {
            [_captureSession addOutput:self.audioDataOutput];
        }
        //æ·»åŠ å…ƒæ•°æ®è¾“å‡º
        BOOL isCameraAvalible = [self checkCameraAuthorization];
        if (isCameraAvalible) {
            if ([_captureSession canAddOutput:self.metadataOutput]) {
                [_captureSession addOutput:self.metadataOutput];
                self.metadataOutput.metadataObjectTypes = @[AVMetadataObjectTypeFace];
            }
        }
        //è®¾ç½®è§†é¢‘å½•åˆ¶çš„æ–¹å‘
        self.videoConnection.videoOrientation = AVCaptureVideoOrientationLandscapeRight;
    }
    return _captureSession;
}
#pragma mark - è§†é¢‘å½•åˆ¶ç›¸å…³è®¿é—®æƒé™æ£€æµ‹ -
- (BOOL)checkCameraAuthorization {
    __block BOOL isAvalible = NO;
    AVAuthorizationStatus status = [AVCaptureDevice authorizationStatusForMediaType:AVMediaTypeVideo];
    switch (status) {
        case AVAuthorizationStatusAuthorized: //æˆæƒ
            isAvalible = YES;
            break;
        case AVAuthorizationStatusDenied:   //æ‹’ç»ï¼Œå¼¹æ¡†
        {
            isAvalible = NO;
        }
            break;
        case AVAuthorizationStatusNotDetermined:   //æ²¡æœ‰å†³å®šï¼Œç¬¬ä¸€æ¬¡å¯åŠ¨é»˜è®¤å¼¹æ¡†
        {
            isAvalible = NO;
        }
            break;
        case AVAuthorizationStatusRestricted:  //å—é™åˆ¶ï¼Œå®¶é•¿æ§åˆ¶å™¨
            isAvalible = NO;
            break;
    }
    return isAvalible;
}

#pragma mark - Recorderå½•åˆ¶ä¼šè¯ è¾“å…¥ é…ç½® -
//åç½®æ‘„åƒå¤´è¾“å…¥
- (AVCaptureDeviceInput *)backCameraInput {
    if (_backCameraInput == nil) {
        NSError *error;
        AVCaptureDevice *backCamera = [self cameraWithPosition:AVCaptureDevicePositionBack];
        _backCameraInput = [[AVCaptureDeviceInput alloc] initWithDevice:backCamera error:&error];
        
        AVCaptureDevice *device = _backCameraInput.device;
        if (device.isSmoothAutoFocusSupported) {
            
            NSError *error;
            if ([device lockForConfiguration:&error]) {
                device.smoothAutoFocusEnabled = YES;
                [device unlockForConfiguration];
            }
        }
        
        if (error) {
            DLYLog(@"è·å–åç½®æ‘„åƒå¤´å¤±è´¥~");
        }
            
//        DLYMobileDevice *mobileDevice = [DLYMobileDevice sharedDevice];
//        DLYPhoneDeviceType phoneType = [mobileDevice iPhoneType];
//
//        if (phoneType == PhoneDeviceTypeIphone_7 || phoneType == PhoneDeviceTypeIphone_7_Plus || phoneType == PhoneDeviceTypeIphone_6s || phoneType == PhoneDeviceTypeIphone_6s_Plus || phoneType == PhoneDeviceTypeIphone_SE) {
//            self.captureSession.sessionPreset = AVCaptureSessionPreset3840x2160;
//        }else{
            self.captureSession.sessionPreset = AVCaptureSessionPreset1280x720;
//        }
    }
    return _backCameraInput;
}
//å‰ç½®æ‘„åƒå¤´è¾“å…¥
- (AVCaptureDeviceInput *)frontCameraInput {
    if (_frontCameraInput == nil) {
        NSError *error;
        
        AVCaptureDevice *frontCamera = [self cameraWithPosition:AVCaptureDevicePositionFront];
        _frontCameraInput = [[AVCaptureDeviceInput alloc] initWithDevice:frontCamera error:&error];
        AVCaptureDevice *device = _frontCameraInput.device;
        
        if (device.isSmoothAutoFocusSupported) {
            
            NSError *error;
            if ([device lockForConfiguration:&error]) {
                device.smoothAutoFocusEnabled = YES;
                [device unlockForConfiguration];
            }
        }
        if (error) {
            DLYLog(@"è·å–å‰ç½®æ‘„åƒå¤´å¤±è´¥~");
        }
        self.captureSession.sessionPreset = AVCaptureSessionPreset1280x720;
    }
    return _frontCameraInput;
}
//éº¦å…‹é£è¾“å…¥
- (AVCaptureDeviceInput *)audioMicInput {
    if (_audioMicInput == nil) {
        AVCaptureDevice *mic = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeAudio];
        NSError *error;
        _audioMicInput = [AVCaptureDeviceInput deviceInputWithDevice:mic error:&error];
        if (error) {
            DLYLog(@"è·å–éº¦å…‹é£å¤±è´¥~");
        }
    }
    return _audioMicInput;
}

#pragma mark - Recorderå½•åˆ¶ä¼šè¯ è¾“å‡º é…ç½® -
//è§†é¢‘è¾“å‡º
- (AVCaptureVideoDataOutput *)videoDataOutput {
    if (_videoDataOutput == nil) {
        _videoDataOutput = [[AVCaptureVideoDataOutput alloc] init];
        NSDictionary* setcapSettings = [NSDictionary dictionaryWithObjectsAndKeys:
                                        [NSNumber numberWithInt:kCVPixelFormatType_32BGRA], kCVPixelBufferPixelFormatTypeKey,nil];
        _videoDataOutput.videoSettings = setcapSettings;
        dispatch_queue_t videoCaptureQueue = dispatch_queue_create("videoDataOutput", DISPATCH_QUEUE_SERIAL);
        [_videoDataOutput setSampleBufferDelegate:self queue:videoCaptureQueue];
        [_videoDataOutput setAlwaysDiscardsLateVideoFrames:YES];
    }
    return _videoDataOutput;
}
//å…ƒæ•°æ®è¾“å‡º
-(AVCaptureMetadataOutput *)metadataOutput {
    if (_metadataOutput == nil) {
        _metadataOutput = [[AVCaptureMetadataOutput alloc]init];
        dispatch_queue_t metadataOutputQueue = dispatch_queue_create("MetadataOutput", DISPATCH_QUEUE_SERIAL);
        [_metadataOutput setMetadataObjectsDelegate:self queue:metadataOutputQueue];
    }
    return _metadataOutput;
}
//éŸ³é¢‘è¾“å‡º
- (AVCaptureAudioDataOutput *)audioDataOutput {
    if (_audioDataOutput == nil) {
        _audioDataOutput = [[AVCaptureAudioDataOutput alloc] init];
        //        [_audioDataOutput setSampleBufferDelegate:self queue:self.captureQueue];
        dispatch_queue_t audioCaptureQueue = dispatch_queue_create("Audiocapture", DISPATCH_QUEUE_SERIAL);
        [_audioDataOutput setSampleBufferDelegate:self queue:audioCaptureQueue];
    }
    return _audioDataOutput;
}

#pragma mark - Recorderå½•åˆ¶ä¼šè¯ è¿æ¥ é…ç½® -
//è§†é¢‘è¿æ¥
- (AVCaptureConnection *)videoConnection {
    if (!_videoConnection) {
        _videoConnection = [self.videoDataOutput connectionWithMediaType:AVMediaTypeVideo];
    }
    return _videoConnection;
}

//éŸ³é¢‘è¿æ¥
- (AVCaptureConnection *)audioConnection {
    if (!_audioConnection) {
        _audioConnection = [self.audioDataOutput connectionWithMediaType:AVMediaTypeAudio];
    }
    return _audioConnection;
}

#pragma mark - åˆå§‹åŒ–AVEngine -
- (instancetype)initWithPreviewView:(UIView *)previewView{
    if (self = [super init]) {
        
        [self createTimer];
        _referenceOrientation = (AVCaptureVideoOrientation)UIDeviceOrientationLandscapeLeft;
        
        self.captureSession = [[AVCaptureSession alloc] init];
        
        //æ·»åŠ åç½®æ‘„åƒå¤´çš„è¾“å…¥
        if ([_captureSession canAddInput:self.backCameraInput]) {
            [_captureSession addInput:self.backCameraInput];
            _currentVideoDeviceInput = self.backCameraInput;
        }
        //æ·»åŠ éº¦å…‹é£çš„è¾“å…¥
        if ([_captureSession canAddInput:self.audioMicInput]) {
            [_captureSession addInput:self.audioMicInput];
        }
        
        if (previewView) {
            self.captureVideoPreviewLayer = [[AVCaptureVideoPreviewLayer alloc] initWithSession:self.captureSession];
            self.captureVideoPreviewLayer.frame = previewView.bounds;
            self.captureVideoPreviewLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;
            self.captureVideoPreviewLayer.orientation = UIDeviceOrientationLandscapeLeft; //home button on right
            self.captureVideoPreviewLayer.contentsGravity = kCAGravityTopLeft;
            [previewView.layer addSublayer:self.captureVideoPreviewLayer];
        }
        //æ·»åŠ è§†é¢‘è¾“å‡º
        if ([_captureSession canAddOutput:self.videoDataOutput]) {
            [_captureSession addOutput:self.videoDataOutput];
        }else{
            NSLog(@"Video output creation faild");
        }
        //æ·»åŠ éŸ³é¢‘è¾“å‡º
        if ([_captureSession canAddOutput:self.audioDataOutput]) {
            [_captureSession addOutput:self.audioDataOutput];
        }else{
            NSLog(@"Audio output creation faild");
        }
        
        //è®¾ç½®è§†é¢‘å½•åˆ¶çš„æ–¹å‘
        if ([self.videoConnection isVideoOrientationSupported]) {
            [self.videoConnection setVideoOrientation:AVCaptureVideoOrientationLandscapeRight];
        }
        
        // Video
        _movieWritingQueue = dispatch_queue_create("moviewriting", DISPATCH_QUEUE_SERIAL);
        _videoOrientation = [self.videoConnection videoOrientation];
        
        // BufferQueue
        OSStatus err = CMBufferQueueCreate(kCFAllocatorDefault, 1, CMBufferQueueGetCallbacksForUnsortedSampleBuffers(), &_previewBufferQueue);
        NSLog(@"CMBufferQueueCreate error:%d", err);
        
        // åˆ¤æ–­å½“å‰è§†é¢‘è®¾å¤‡æ˜¯å¦æ”¯æŒå…‰å­¦é˜²æŠ–
        if ([_currentVideoDeviceInput.device.activeFormat isVideoStabilizationModeSupported:AVCaptureVideoStabilizationModeCinematic]) {
            // å¦‚æœæ”¯æŒé˜²æŠ–å°±æ‰“å¼€é˜²æŠ–æ¨¡å¼
            self.videoConnection.preferredVideoStabilizationMode = AVCaptureVideoStabilizationModeCinematic;
        }
        
        //è§†é¢‘å½•åˆ¶é˜Ÿåˆ—
        _movieWritingQueue = dispatch_queue_create("moviewriting", DISPATCH_QUEUE_SERIAL);
        _videoOrientation = [self.videoConnection videoOrientation];
        
        self.metadataOutput.rectOfInterest = [self.captureVideoPreviewLayer metadataOutputRectOfInterestForRect:CGRectMake(0, 0, SCREEN_WIDTH, SCREEN_HEIGHT)];
        
        [self.captureSession startRunning];
    }
    return self;
}

#pragma mark - åˆ‡æ¢æ‘„åƒå¤´ -
- (void)changeCameraInputDeviceisFront:(BOOL)isFront {
    
    if (isFront) {
        [self.captureSession beginConfiguration];
        [self.captureSession removeInput:self.backCameraInput];
        
        if ([self.captureSession canAddInput:self.frontCameraInput]) {
            [self changeCameraAnimation];
            [self.captureSession addInput:self.frontCameraInput];//åˆ‡æ¢æˆäº†å‰ç½®
            _currentVideoDeviceInput = self.frontCameraInput;
        }
    }else {
        [self.captureSession beginConfiguration];
        [self.captureSession removeInput:self.frontCameraInput];
        if ([self.captureSession canAddInput:self.backCameraInput]) {
            [self changeCameraAnimation];
            [self.captureSession addInput:self.backCameraInput];//åˆ‡æ¢æˆäº†åç½®
            _currentVideoDeviceInput = self.frontCameraInput;
        }
    }
    [self.captureSession commitConfiguration];
}

//ç”¨æ¥è¿”å›æ˜¯å‰ç½®æ‘„åƒå¤´è¿˜æ˜¯åç½®æ‘„åƒå¤´
- (AVCaptureDevice *)cameraWithPosition:(AVCaptureDevicePosition) position {
    
    NSArray *devices = [AVCaptureDevice devicesWithMediaType:AVMediaTypeVideo];
    
    for (AVCaptureDevice *device in devices) {
        if ([device position] == position) {
            
            _defaultFormat = device.activeFormat;
            _defaultMinFrameDuration = device.activeVideoMinFrameDuration;
            _defaultMaxFrameDuration = device.activeVideoMaxFrameDuration;
            DLYLog(@"å½“å‰é€‰æ‹©çš„device.activeFormat :",_defaultFormat);
            return device;
        }
    }
    return nil;
}
//æ‘„åƒå¤´åˆ‡æ¢ç¿»è½¬åŠ¨ç”»
- (void)changeCameraAnimation {
    CATransition *changeAnimation = [CATransition animation];
    changeAnimation.delegate = self;
    changeAnimation.duration = 0.3;
    changeAnimation.type = @"oglFlip";
    changeAnimation.subtype = kCATransitionFromTop;
    [self.captureVideoPreviewLayer addAnimation:changeAnimation forKey:@"changeAnimation"];
}
//é¡ºæ—¶é’ˆæ—‹è½¬
- (void)changeCameraRotateClockwiseAnimation {
    CABasicAnimation *animation =  [CABasicAnimation animationWithKeyPath:@"transform.rotation.z"];
    //é»˜è®¤æ˜¯é¡ºæ—¶é’ˆæ•ˆæœï¼Œè‹¥å°†fromValueå’ŒtoValueçš„å€¼äº’æ¢ï¼Œåˆ™ä¸ºé€†æ—¶é’ˆæ•ˆæœ
    animation.fromValue = [NSNumber numberWithFloat:0.f];
    animation.toValue =  [NSNumber numberWithFloat: M_PI];
    animation.duration  = 0.2;
    animation.autoreverses = NO;
    animation.fillMode =kCAFillModeForwards;
    animation.repeatCount = 0;
    [self.captureVideoPreviewLayer addAnimation:animation forKey:nil];
}

//é€†æ—¶é’ˆæ—‹è½¬
- (void)changeCameraRotateAnticlockwiseAnimation {
    CABasicAnimation *animation =  [CABasicAnimation animationWithKeyPath:@"transform.rotation.z"];
    //é»˜è®¤æ˜¯é¡ºæ—¶é’ˆæ•ˆæœï¼Œè‹¥å°†fromValueå’ŒtoValueçš„å€¼äº’æ¢ï¼Œåˆ™ä¸ºé€†æ—¶é’ˆæ•ˆæœ
    animation.fromValue = [NSNumber numberWithFloat: M_PI];
    animation.toValue = [NSNumber numberWithFloat:0.f];
    animation.duration  = 0.2;
    animation.autoreverses = NO;
    animation.fillMode = kCAFillModeForwards;
    animation.repeatCount = 0;
    [self.captureVideoPreviewLayer addAnimation:animation forKey:nil];
}
- (void)animationDidStart:(CAAnimation *)anim {
    [self.captureSession startRunning];
}

#pragma mark - ç‚¹è§¦è®¾ç½®æ›å…‰ -

CGFloat distanceBetweenPoints (CGPoint first, CGPoint second) {
    CGFloat deltaX = second.x - first.x;
    CGFloat deltaY = second.y - first.y;
    return sqrt(deltaX*deltaX + deltaY*deltaY);
};

- (void)focusOnceWithPoint:(CGPoint)point{
    
    AVCaptureDevice *captureDevice = _currentVideoDeviceInput.device;
    
    if ([captureDevice lockForConfiguration:nil]) {
        
        // è®¾ç½®å¯¹ç„¦
        if ([captureDevice isFocusModeSupported:AVCaptureFocusModeLocked]) {
            [captureDevice setFocusMode:AVCaptureFocusModeLocked];
        }
        if ([captureDevice isFocusPointOfInterestSupported]) {
            [captureDevice setFocusPointOfInterest:point];
        }
        
        // è®¾ç½®æ›å…‰
        if ([captureDevice isExposureModeSupported:AVCaptureExposureModeLocked]) {
            [captureDevice setExposureMode:AVCaptureExposureModeLocked];
        }
        if ([captureDevice isExposurePointOfInterestSupported]) {
            [captureDevice setExposurePointOfInterest:point];
        }
        
        //è®¾ç½®ç™½å¹³è¡¡
        if ([captureDevice isWhiteBalanceModeSupported:AVCaptureWhiteBalanceModeLocked]) {
            [captureDevice setWhiteBalanceMode:AVCaptureWhiteBalanceModeLocked];
        }
        [captureDevice unlockForConfiguration];
    }
}

-(void)focusWithMode:(AVCaptureFocusMode)focusMode atPoint:(CGPoint)point{
    
    AVCaptureDevice *captureDevice = _currentVideoDeviceInput.device;
    
    if ([captureDevice lockForConfiguration:nil]) {
        
        // è®¾ç½®å¯¹ç„¦
        if ([captureDevice isFocusModeSupported:AVCaptureFocusModeContinuousAutoFocus]) {
            [captureDevice setFocusMode:AVCaptureFocusModeContinuousAutoFocus];
        }
        if ([captureDevice isFocusPointOfInterestSupported]) {
            [captureDevice setFocusPointOfInterest:point];
        }
        
        // è®¾ç½®æ›å…‰
        if ([captureDevice isExposureModeSupported:AVCaptureExposureModeContinuousAutoExposure]) {
            [captureDevice setExposureMode:AVCaptureExposureModeContinuousAutoExposure];
        }
        if ([captureDevice isExposurePointOfInterestSupported]) {
            [captureDevice setExposurePointOfInterest:point];
        }
        
        //è®¾ç½®ç™½å¹³è¡¡
        if ([captureDevice isWhiteBalanceModeSupported:AVCaptureWhiteBalanceModeAutoWhiteBalance]) {
            [captureDevice setWhiteBalanceMode:AVCaptureWhiteBalanceModeAutoWhiteBalance];
        }
        [captureDevice unlockForConfiguration];
        
        NSLog(@"Current point of the capture device is :x = %f,y = %f",point.x,point.y);
    }
}

-(void)focusAtPoint:(CGPoint)point{
    
    [self changeDeviceProperty:^(AVCaptureDevice *captureDevice) {
        
    }];
}

-(void)changeDeviceProperty:(void(^)(AVCaptureDevice *captureDevice))propertyChange{
    
    AVCaptureDevice *captureDevice= [_currentVideoDeviceInput device];
    NSError *error;
    
    if ([captureDevice lockForConfiguration:&error]) {
        
        propertyChange(captureDevice);
        [captureDevice unlockForConfiguration];
        
    }else{
        NSLog(@"è®¾ç½®è®¾å¤‡å±æ€§è¿‡ç¨‹å‘ç”Ÿé”™è¯¯ï¼Œé”™è¯¯ä¿¡æ¯ï¼š%@",error.localizedDescription);
    }
}

- (void)resetFormat {
    
    BOOL isRunning = self.captureSession.isRunning;
    
    if (isRunning) {
        [self.captureSession beginConfiguration];
    }
    
    [_currentVideoDeviceInput.device lockForConfiguration:nil];
    
    _currentVideoDeviceInput.device.activeFormat = self.defaultFormat;
    _currentVideoDeviceInput.device.activeVideoMaxFrameDuration = _defaultMaxFrameDuration;
    _currentVideoDeviceInput.device.activeVideoMinFrameDuration = _defaultMinFrameDuration;
    
    [_currentVideoDeviceInput.device  unlockForConfiguration];
    
    if (isRunning) {
        [self.captureSession commitConfiguration];
    }
}

#pragma mark - å¼€å§‹å½•åˆ¶ -
- (void)startRecordingWithPart:(DLYMiniVlogPart *)part {
    _currentPart = part;
    
    UIDeviceOrientation deviceOriention = [[UIDevice currentDevice] orientation];
    
    NSLog(@"deviceOriention :%lu",deviceOriention);
    
    if (!self.isCapturing) {
        self.isCapturing = YES;
    }
    NSString *_outputPath;

    if (_currentPart.recordType == DLYMiniVlogRecordTypeSlomo) {
        DLYLog(@"ğŸ¬ğŸ¬ğŸ¬Record Type Is Slomo");
        [self cameraBackgroundDidClickOpenSlow];
        
    }else if (_currentPart.recordType == DLYMiniVlogRecordTypeTimelapse){
        DLYLog(@"ğŸ¬ğŸ¬ğŸ¬Record Type Is Timelapse");
        [self cameraBackgroundDidClickCloseSlow];
    }else{
        DLYLog(@"ğŸ¬ğŸ¬ğŸ¬Record Type Is Normal");
        [self cameraBackgroundDidClickCloseSlow];
    }
    _outputPath = [self.resource getSaveDraftPartWithPartNum:_currentPart.partNum];
    if (_outputPath) {
        _currentPart.partUrl = [NSURL fileURLWithPath:_outputPath];
        DLYLog(@"ç¬¬ %lu ä¸ªç‰‡æ®µçš„åœ°å€ :%@",_currentPart.partNum + 1,_currentPart.partUrl);
    }else{
        DLYLog(@"ç‰‡æ®µåœ°å€è·å–ä¸ºç©º");
    }
    
    NSError *error;
    self.assetWriter = [[AVAssetWriter alloc] initWithURL:_currentPart.partUrl fileType:AVFileTypeMPEG4 error:&error];
    if (error) {
        DLYLog(@"AVAssetWriter error:%@", error);
    }
    _recordingWillBeStarted = YES;
}
#pragma mark - åœæ­¢å½•åˆ¶ -
- (void)stopRecording {
    
//    if(_assetWriter && _assetWriter.status == AVAssetWriterStatusWriting){
        dispatch_async(_movieWritingQueue, ^{

            _isRecording = NO;
            _readyToRecordVideo = NO;
            _readyToRecordAudio = NO;

            [self.assetWriter finishWritingWithCompletionHandler:^{

                self.assetWriterVideoInput = nil;
                self.assetWriterAudioInput = nil;
                self.assetWriter = nil;

                [self saveRecordedFileByUrl:_currentPart.partUrl];
                dispatch_async(dispatch_get_main_queue(), ^{

                    if ([self.delegate respondsToSelector:@selector(didFinishRecordingToOutputFileAtURL:error:)]) {
                        [self.delegate didFinishRecordingToOutputFileAtURL:_currentPart.partUrl error:nil];
                    }
                });
            }];
        });
//    }
}
#pragma mark - å–æ¶ˆå½•åˆ¶ -
- (void)cancelRecording{
    dispatch_async(_movieWritingQueue, ^{
        
        _isRecording = NO;
        _readyToRecordVideo = NO;
        _readyToRecordAudio = NO;
        
        [self.assetWriter finishWritingWithCompletionHandler:^{
            
            self.assetWriterVideoInput = nil;
            self.assetWriterAudioInput = nil;
            self.assetWriter = nil;
        }];
    });
}
#pragma mark - æš‚åœå½•åˆ¶ -
- (void) pauseRecording{
    if (self.captureSession.isRunning) {
        [self.captureSession stopRunning];
    }
}
- (void) saveRecordedFileByUrl:(NSURL *)saveUrl
{
    if (_isRecordingCancel) {
        _isRecordingCancel = NO;
        DLYLog(@"å–æ¶ˆå½•åˆ¶");
    }else{
        
        //å¿«æ…¢é•œå¤´è°ƒé€Ÿä¹‹åè·å–ä¿å­˜åœ¨Documentä¸­åœ°å€
        NSString *exportPath;
        NSString *dataPath = [kPathDocument stringByAppendingPathComponent:kDataFolder];
        
        if ([[NSFileManager defaultManager] fileExistsAtPath:dataPath]) {
            NSString *draftPath = [dataPath stringByAppendingPathComponent:kDraftFolder];
            if ([[NSFileManager defaultManager] fileExistsAtPath:draftPath]) {
                exportPath = [NSString stringWithFormat:@"%@/part%lu.mp4",draftPath,(long)_currentPart.partNum];
            }
        }
        NSURL *exportUrl = [NSURL fileURLWithPath:exportPath];
        
        dispatch_async(dispatch_get_main_queue(), ^{
            [[DLYIndicatorView sharedIndicatorView] startFlashAnimatingWithTitle:@"ç‰‡æ®µå¤„ç†ä¸­..."];
            typeof(self) weakSelf = self;
            [weakSelf setSpeedWithVideo:_currentPart.partUrl outputUrl:exportUrl recordTypeOfPart:_currentPart.recordType completed:^{
                DLYLog(@"ç¬¬ %lu ä¸ªç‰‡æ®µè°ƒé€Ÿå®Œæˆ",self.currentPart.partNum + 1);
                [self.resource removePartWithPartNumFormCache:self.currentPart.partNum];
                dispatch_async(dispatch_get_main_queue(), ^{
                    [[DLYIndicatorView sharedIndicatorView] stopFlashAnimating];
                });
            }];
        });
    }
}
#pragma mark - è§†é¢‘é€Ÿåº¦å¤„ç† -

// å¤„ç†é€Ÿåº¦è§†é¢‘
- (void)setSpeedWithVideo:(NSURL *)videoPartUrl outputUrl:(NSURL *)outputUrl recordTypeOfPart:(DLYMiniVlogRecordType)recordType completed:(void(^)())completed {
    
    NSLog(@"video set thread: %@", [NSThread currentThread]);
    NSLog(@"å¤„ç†è§†é¢‘é€Ÿåº¦ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€");
    // è·å–è§†é¢‘
    if (!videoPartUrl) {
        DLYLog(@"å¾…è°ƒé€Ÿçš„è§†é¢‘ç‰‡æ®µä¸å­˜åœ¨!");
        return;
    }else{
        
        // é€‚é…è§†é¢‘é€Ÿåº¦æ¯”ç‡
        CGFloat scale = 0;
        if(recordType == DLYMiniVlogRecordTypeTimelapse){
            scale = 0.2f;  // 0.2å¯¹åº”  å¿«é€Ÿ x5   æ’­æ”¾æ—¶é—´å‹ç¼©å¸§ç‡å¹³å‡(ä½å¸§ç‡)
        } else if (recordType == DLYMiniVlogRecordTypeSlomo) {
            scale = 3.0f;  // æ…¢é€Ÿ x3   æ’­æ”¾æ—¶é—´æ‹‰é•¿å¸§ç‡å¹³å‡(é«˜å¸§ç‡)
        }else{
            scale = 1.0f;
        }
        AVURLAsset *videoAsset = nil;
        if(videoPartUrl) {
            videoAsset = [[AVURLAsset alloc]initWithURL:videoPartUrl options:nil];
        }
        AVAssetTrack *videoAssetTrack = nil;
        if([videoAsset tracksWithMediaType:AVMediaTypeVideo]){
            videoAssetTrack = [[videoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
        }
        CGAffineTransform videoTransform = videoAssetTrack.preferredTransform;
        
        NSLog(@"preferredTransform a = %.0f     b = %.0f       c = %.0f     d = %.0f,       tx = %.0f       ty = %.0f",videoTransform.a,videoTransform.b,videoTransform.c,videoTransform.d,videoTransform.tx,videoTransform.ty);
        // è§†é¢‘æ··åˆ
        AVMutableComposition* mixComposition = [AVMutableComposition composition];
        // è§†é¢‘è½¨é“
        AVMutableCompositionTrack *compositionVideoTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
        
//        if (videoTransform.a == 0 && videoTransform.b == 1 && videoTransform.c == -1 && videoTransform.d == 0) {
//            compositionVideoTrack.preferredTransform = CGAffineTransformMakeRotation(M_PI);
//        }

        if (recordType == DLYMiniVlogRecordTypeNormal) {
            NSError *error = nil;
            NSFileManager *fileManager = [NSFileManager defaultManager];
            BOOL isSuccess = [fileManager moveItemAtURL:videoPartUrl toURL:outputUrl error:&error];
            DLYLog(@"%@",isSuccess ? @"ç§»åŠ¨ä¸éœ€è¦è°ƒé€Ÿçš„è§†é¢‘ç‰‡æ®µæˆåŠŸ":@"ç§»åŠ¨ä¸éœ€è¦è°ƒé€Ÿçš„é¢‘æ®µç‰‡æ®µå¤±è´¥");
            dispatch_async(dispatch_get_main_queue(), ^{
                [[DLYIndicatorView sharedIndicatorView] stopFlashAnimating];
            });
            // éŸ³é¢‘è½¨é“
            AVMutableCompositionTrack *compositionAudioTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];
            
            // æ’å…¥è§†é¢‘è½¨é“
            [compositionVideoTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeMake(videoAsset.duration.value, videoAsset.duration.timescale)) ofTrack:[[videoAsset tracksWithMediaType:AVMediaTypeVideo] firstObject] atTime:kCMTimeZero error:nil];
            // æ’å…¥éŸ³é¢‘è½¨é“
            [compositionAudioTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeMake(videoAsset.duration.value, videoAsset.duration.timescale)) ofTrack:[[videoAsset tracksWithMediaType:AVMediaTypeAudio] firstObject] atTime:kCMTimeZero error:nil];

        }else{//å¿«æ…¢é•œå¤´ä¸¢å¼ƒåŸå§‹éŸ³é¢‘
        
            // æ’å…¥è§†é¢‘è½¨é“
            [compositionVideoTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeMake(videoAsset.duration.value, videoAsset.duration.timescale)) ofTrack:[[videoAsset tracksWithMediaType:AVMediaTypeVideo] firstObject] atTime:kCMTimeZero error:nil];
            
            // æ ¹æ®é€Ÿåº¦æ¯”ç‡è°ƒèŠ‚éŸ³é¢‘å’Œè§†é¢‘
            [compositionVideoTrack scaleTimeRange:CMTimeRangeMake(kCMTimeZero, CMTimeMake(videoAsset.duration.value, videoAsset.duration.timescale)) toDuration:CMTimeMake(videoAsset.duration.value * scale , videoAsset.duration.timescale)];
        }
        // é…ç½®å¯¼å‡º
        AVAssetExportSession *assetExport = [[AVAssetExportSession alloc] initWithAsset:mixComposition presetName:AVAssetExportPreset1280x720];
        
        assetExport.outputFileType = AVFileTypeMPEG4;
        assetExport.outputURL = outputUrl;
        assetExport.shouldOptimizeForNetworkUse = YES;
        
        // å¯¼å‡ºè§†é¢‘
        [assetExport exportAsynchronouslyWithCompletionHandler:^{
            completed();
        }];
    }
}
#pragma mark - æ‰“å¼€æ…¢åŠ¨ä½œå½•åˆ¶ -
- (void)cameraBackgroundDidClickOpenSlow {
    
    [self.captureSession stopRunning];
    CGFloat desiredFPS = 120.0;
    NSLog(@"å½“å‰è®¾ç½®çš„å½•åˆ¶å¸§ç‡æ˜¯: %f",desiredFPS);
    AVCaptureDeviceFormat *selectedFormat = nil;
    int32_t maxWidth = 0;
    AVFrameRateRange *frameRateRange = nil;
    for (AVCaptureDeviceFormat *format in [_currentVideoDeviceInput.device formats]) {
        for (AVFrameRateRange *range in format.videoSupportedFrameRateRanges) {
            CMFormatDescriptionRef desc = format.formatDescription;
            CMVideoDimensions dimensions = CMVideoFormatDescriptionGetDimensions(desc);
            int32_t width = dimensions.width;
            if (range.minFrameRate <= desiredFPS && desiredFPS <= range.maxFrameRate && width >= maxWidth) {
                selectedFormat = format;
                frameRateRange = range;
                maxWidth = width;
            }
        }
    }
    if (selectedFormat) {
        if ([_currentVideoDeviceInput.device lockForConfiguration:nil]) {
            NSLog(@"selected format: %@", selectedFormat);
            _currentVideoDeviceInput.device.activeFormat = selectedFormat;
            _currentVideoDeviceInput.device.activeVideoMinFrameDuration = CMTimeMake(1, (int32_t)desiredFPS);
            _currentVideoDeviceInput.device.activeVideoMaxFrameDuration = CMTimeMake(1, (int32_t)desiredFPS);
            [_currentVideoDeviceInput.device unlockForConfiguration];
        }
    }
    [self.captureSession startRunning];
}
#pragma mark - å…³é—­æ…¢åŠ¨ä½œå½•åˆ¶ -
- (void)cameraBackgroundDidClickCloseSlow {
    
    [self.captureSession stopRunning];
    CGFloat desiredFPS = 60.0f;
    
    NSLog(@"å½“å‰è®¾ç½®çš„å½•åˆ¶å¸§ç‡æ˜¯: %f",desiredFPS);
    AVCaptureDeviceFormat *selectedFormat = nil;
    int32_t maxWidth = 0;
    AVFrameRateRange *frameRateRange = nil;
    
    for (AVCaptureDeviceFormat *format in [_currentVideoDeviceInput.device formats]) {
        for (AVFrameRateRange *range in format.videoSupportedFrameRateRanges) {
            CMFormatDescriptionRef desc = format.formatDescription;
            CMVideoDimensions dimensions = CMVideoFormatDescriptionGetDimensions(desc);
            int32_t width = dimensions.width;
            if (range.minFrameRate <= desiredFPS && desiredFPS <= range.maxFrameRate && width >= maxWidth) {
                selectedFormat = format;
                frameRateRange = range;
                maxWidth = width;
            }
        }
    }
    if (selectedFormat) {
        if ([_currentVideoDeviceInput.device lockForConfiguration:nil]) {
            NSLog(@"selected format: %@", selectedFormat);
//            _captureDeviceInput.device.activeFormat = _defaultFormat;
//            _captureDeviceInput.device.activeVideoMinFrameDuration = _defaultMinFrameDuration;
//            _captureDeviceInput.device.activeVideoMaxFrameDuration = _defaultMaxFrameDuration;
//            [_captureDeviceInput.device unlockForConfiguration];
            _currentVideoDeviceInput.device.activeFormat = selectedFormat;
            _currentVideoDeviceInput.device.activeVideoMinFrameDuration = CMTimeMake(1, (int32_t)desiredFPS);
            _currentVideoDeviceInput.device.activeVideoMaxFrameDuration = CMTimeMake(1, (int32_t)desiredFPS);
            [_currentVideoDeviceInput.device unlockForConfiguration];
        }
    }
    [self.captureSession startRunning];
}
#pragma mark - å†…éƒ¨å¤„ç†æ–¹æ³•
- (NSString *)movieName {
    NSDate *datenow = [NSDate date];
    NSString *timeSp = [NSString stringWithFormat:@"time_%ld", (long)[datenow timeIntervalSince1970]];
    return [timeSp stringByAppendingString:@".mov"];
}
#pragma mark - é‡ç½®å½•åˆ¶session -
- (void) restartRecording{
    if (!self.captureSession.isRunning) {
        [self.captureSession startRunning];
    }
}

#pragma mark - AVCaptureFileOutputRecordingDelegate -
-(void)captureOutput:(AVCaptureFileOutput *)captureOutput didStartRecordingToOutputFileAtURL:(NSURL *)fileURL fromConnections:(NSArray *)connections{
    DLYLog(@"å¼€å§‹å½•åˆ¶,æ­£åœ¨å†™å…¥...");
}
-(void)captureOutput:(AVCaptureFileOutput *)captureOutput didFinishRecordingToOutputFileAtURL:(NSURL *)outputFileURL fromConnections:(NSArray *)connections error:(NSError *)error{
    DLYLog(@"ç»“æŸå½•åˆ¶,å†™å…¥å®Œæˆ!!!");
}
#pragma mark - AVCaptureVideoDataOutputSampleBufferDelegate
- (void)captureOutput:(AVCaptureOutput *)captureOutput didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection
{
    if (self.onBuffer) {
        self.onBuffer(sampleBuffer);
    }
    CMFormatDescriptionRef formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer);
    
    CFRetain(sampleBuffer);
    
    dispatch_async(_movieWritingQueue, ^{
        
        if (self.assetWriter && (self.isRecording || _recordingWillBeStarted)) {
            
            BOOL wasReadyToRecord = (_readyToRecordAudio && _readyToRecordVideo);
            
            if (connection == self.videoConnection) {
                // Initialize the video input if this is not done yet
                if (!_readyToRecordVideo) {
                    _readyToRecordVideo = [self setupAssetWriterVideoInput:formatDescription];
                }
                
                // Write video data to file
                if (_readyToRecordVideo && _readyToRecordAudio) {
                    [self writeSampleBuffer:sampleBuffer ofType:AVMediaTypeVideo];
                }
            }
            else if (connection == self.audioConnection) {
                // Initialize the audio input if this is not done yet
                if (!_readyToRecordAudio) {
                    _readyToRecordAudio = [self setupAssetWriterAudioInput:formatDescription];
                }
                
                // Write audio data to file
                if (_readyToRecordAudio && _readyToRecordVideo)
                    [self writeSampleBuffer:sampleBuffer ofType:AVMediaTypeAudio];
            }
            
            BOOL isReadyToRecord = (_readyToRecordAudio && _readyToRecordVideo);
            if (!wasReadyToRecord && isReadyToRecord) {
                _recordingWillBeStarted = NO;
                _isRecording = YES;
            }
        }
        CFRelease(sampleBuffer);
    });
}
- (void)writeSampleBuffer:(CMSampleBufferRef)sampleBuffer ofType:(NSString *)mediaType
{
    if (self.assetWriter.status == AVAssetWriterStatusUnknown) {
        
        if ([self.assetWriter startWriting]) {
            
            CMTime timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);
            [self.assetWriter startSessionAtSourceTime:timestamp];
        }
        else {
            if (self.assetWriter.error) {
                DLYLog(@"AVAssetWriter startWriting error:%@", self.assetWriter.error);
            }
        }
    }
    
    if (self.assetWriter.status == AVAssetWriterStatusWriting) {
        
        if (mediaType == AVMediaTypeVideo) {
            
            if (self.assetWriterVideoInput.readyForMoreMediaData) {
                
                if (![self.assetWriterVideoInput appendSampleBuffer:sampleBuffer]) {
                    
                    DLYLog(@"isRecording:%d, willBeStarted:%d", self.isRecording, _recordingWillBeStarted);
                    if (self.assetWriter.error) {
                        DLYLog(@"AVAssetWriterInput video appendSampleBuffer error:%@", self.assetWriter.error);
                    }
                }
            }
        }
        else if (mediaType == AVMediaTypeAudio) {
            
            if (self.assetWriterAudioInput.readyForMoreMediaData) {
                
                if (![self.assetWriterAudioInput appendSampleBuffer:sampleBuffer]) {
                    if (self.assetWriter.error) {
                        DLYLog(@"AVAssetWriterInput audio appendSapleBuffer error:%@", self.assetWriter.error);
                    }
                }
            }
        }
    }
}
#pragma mark -è§†é¢‘æ•°æ®è¾“å‡ºè®¾ç½®-

- (BOOL)setupAssetWriterVideoInput:(CMFormatDescriptionRef)currentFormatDescription
{
    float bitsPerPixel;
    CMVideoDimensions dimensions = CMVideoFormatDescriptionGetDimensions(currentFormatDescription);
    int numPixels = dimensions.width * dimensions.height;
    int bitsPerSecond;
    
    // Assume that lower-than-SD resolutions are intended for streaming, and use a lower bitrate
    if ( numPixels < (640 * 480) )
        bitsPerPixel = 4.05; // This bitrate matches the quality produced by AVCaptureSessionPresetMedium or Low.
    else
        bitsPerPixel = 11.4; // This bitrate matches the quality produced by AVCaptureSessionPresetHigh.
    
    bitsPerSecond = numPixels * bitsPerPixel;
    
    NSDictionary *videoCompressionSettings = [NSDictionary dictionaryWithObjectsAndKeys:
                                              AVVideoCodecH264, AVVideoCodecKey,
                                              [NSNumber numberWithInteger:dimensions.width], AVVideoWidthKey,
                                              [NSNumber numberWithInteger:dimensions.height], AVVideoHeightKey,
                                              [NSDictionary dictionaryWithObjectsAndKeys:
                                               [NSNumber numberWithInteger:bitsPerSecond], AVVideoAverageBitRateKey,
                                               [NSNumber numberWithInteger:30], AVVideoMaxKeyFrameIntervalKey,
                                               nil], AVVideoCompressionPropertiesKey,
                                              nil];
    
    if ([self.assetWriter canApplyOutputSettings:videoCompressionSettings forMediaType:AVMediaTypeVideo]) {
        
        self.assetWriterVideoInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeVideo outputSettings:videoCompressionSettings];
        self.assetWriterVideoInput.expectsMediaDataInRealTime = YES;
        
        if ([self.assetWriter canAddInput:self.assetWriterVideoInput]) {
            
            [self.assetWriter addInput:self.assetWriterVideoInput];
        }
        else {
            DLYLog(@"Couldn't add asset writer video input.");
            return NO;
        }
    }
    else {
        DLYLog(@"Couldn't apply video output settings.");
        return NO;
    }
    return YES;
}
- (BOOL)setupAssetWriterAudioInput:(CMFormatDescriptionRef)currentFormatDescription
{
    const AudioStreamBasicDescription *currentASBD = CMAudioFormatDescriptionGetStreamBasicDescription(currentFormatDescription);
    
    size_t aclSize = 0;
    const AudioChannelLayout *currentChannelLayout = CMAudioFormatDescriptionGetChannelLayout(currentFormatDescription, &aclSize);
    NSData *currentChannelLayoutData = nil;
    
    if ( currentChannelLayout && aclSize > 0 ) {
        currentChannelLayoutData = [NSData dataWithBytes:currentChannelLayout length:aclSize];
    }
    else {
        currentChannelLayoutData = [NSData data];
    }
    
    NSDictionary *audioCompressionSettings = [NSDictionary dictionaryWithObjectsAndKeys:
                                              [NSNumber numberWithInteger:kAudioFormatMPEG4AAC], AVFormatIDKey,
                                              [NSNumber numberWithFloat:currentASBD->mSampleRate], AVSampleRateKey,
                                              [NSNumber numberWithInt:64000], AVEncoderBitRatePerChannelKey,
                                              [NSNumber numberWithInteger:currentASBD->mChannelsPerFrame], AVNumberOfChannelsKey,
                                              currentChannelLayoutData, AVChannelLayoutKey,
                                              nil];
    if ([self.assetWriter canApplyOutputSettings:audioCompressionSettings forMediaType:AVMediaTypeAudio]) {
        
        self.assetWriterAudioInput = [[AVAssetWriterInput alloc] initWithMediaType:AVMediaTypeAudio
                                                                    outputSettings:audioCompressionSettings];
        self.assetWriterAudioInput.expectsMediaDataInRealTime = YES;
        
        if ([self.assetWriter canAddInput:self.assetWriterAudioInput]) {
            [self.assetWriter addInput:self.assetWriterAudioInput];
        }
        else {
            DLYLog(@"Couldn't add asset writer audio input.");
            return NO;
        }
    }
    else {
        DLYLog(@"Couldn't apply audio output settings.");
        return NO;
    }
    return YES;
}
//è°ƒæ•´åª’ä½“æ•°æ®çš„æ—¶é—´
- (CMSampleBufferRef)adjustTime:(CMSampleBufferRef)sample by:(CMTime)offset {
    CMItemCount count;
    CMSampleBufferGetSampleTimingInfoArray(sample, 0, nil, &count);
    CMSampleTimingInfo* pInfo = malloc(sizeof(CMSampleTimingInfo) * count);
    CMSampleBufferGetSampleTimingInfoArray(sample, count, pInfo, &count);
    for (CMItemCount i = 0; i < count; i++) {
        pInfo[i].decodeTimeStamp = CMTimeSubtract(pInfo[i].decodeTimeStamp, offset);
        pInfo[i].presentationTimeStamp = CMTimeSubtract(pInfo[i].presentationTimeStamp, offset);
    }
    CMSampleBufferRef sout;
    CMSampleBufferCreateCopyWithNewTiming(nil, sample, count, pInfo, &sout);
    free(pInfo);
    return sout;
}

#pragma mark ä»è¾“å‡ºçš„å…ƒæ•°æ®ä¸­æ•æ‰äººè„¸

-(void)captureOutput:(AVCaptureOutput *)captureOutput didOutputMetadataObjects:(NSArray *)metadataObjects fromConnection:(AVCaptureConnection *)connection{
    
    if (metadataObjects.count) {
        isDetectedMetadataObjectTarget = YES;
        AVMetadataMachineReadableCodeObject *metadataObject = metadataObjects.firstObject;
        
        AVMetadataObject *transformedMetadataObject = [self.captureVideoPreviewLayer transformedMetadataObjectForMetadataObject:metadataObject];
        _faceRegion = transformedMetadataObject.bounds;
        
        if (metadataObject.type == AVMetadataObjectTypeFace) {
//            CGRect referenceRect = CGRectMake(0, 0, SCREEN_WIDTH, SCREEN_HEIGHT);
        }else{
            _faceRegion = CGRectZero;
        }
    }else{
        isDetectedMetadataObjectTarget = NO;
        _faceRegion = CGRectZero;
    }
}
NSInteger timeCount = 0;
NSInteger maskCount = 0;
NSInteger startCount = MAXFLOAT;
BOOL isOnce = YES;
- (void)createTimer{
    //è·å¾—é˜Ÿåˆ—
    dispatch_queue_t queue = dispatch_get_global_queue(0, 0);
    //åˆ›å»ºä¸€ä¸ªå®šæ—¶å™¨
    dispatch_source_t enliveTime = dispatch_source_create(DISPATCH_SOURCE_TYPE_TIMER, 0, 0, queue);
    //è®¾ç½®å¼€å§‹æ—¶é—´
    dispatch_time_t start = dispatch_time(DISPATCH_TIME_NOW, (int64_t)(1.0 * NSEC_PER_SEC));
    //è®¾ç½®æ—¶é—´é—´éš”
    uint64_t interval = (uint64_t)(1.0 * NSEC_PER_SEC);
    //è®¾ç½®å®šæ—¶å™¨
    dispatch_source_set_timer(enliveTime, start, interval, 0);
    //è®¾ç½®å›è°ƒ
    dispatch_source_set_event_handler(enliveTime, ^{
        
        CGFloat distance = distanceBetweenPoints(_faceRegion.origin, _lastFaceRegion.origin);
        _lastFaceRegion = _faceRegion;
        if (distance < 20) {
            if (isOnce) {
                isOnce = NO;
                //                CGPoint point = CGPointMake(faceRegion.size.width/2, faceRegion.size.height/2);
                //                CGPoint cameraPoint = [self.previewLayer captureDevicePointOfInterestForPoint:point];
                //                [self focusOnceWithPoint:cameraPoint];
                startCount = timeCount;
            }
            maskCount++;
        }
        timeCount++;
        if (timeCount - startCount >= 2) {
            if (maskCount == 2) {
                _faceRegion = CGRectZero;
            }
            isOnce = YES;
            startCount = MAXFLOAT;
            maskCount = 0;
        }
        dispatch_async(dispatch_get_main_queue(), ^{
            if (self.delegate && [self.delegate respondsToSelector:@selector(displayRefrenceRect:)]) {
                [self.delegate displayRefrenceRect:_faceRegion];
            }
        });
        if(timeCount > MAXFLOAT){
            dispatch_cancel(enliveTime);
        }
        
    });
    //å¯åŠ¨å®šæ—¶å™¨
    dispatch_resume(enliveTime);
}
#pragma mark - è·å–è§†é¢‘æŸä¸€å¸§å›¾åƒ -

-(UIImage*)getKeyImage:(NSURL *)assetUrl intervalTime:(NSInteger)intervalTime{
    
    AVURLAsset *asset = [[AVURLAsset alloc] initWithURL:assetUrl options:nil];
    NSParameterAssert(asset);
    if (!asset) {
        return nil;
    }
    AVAssetImageGenerator *assetImageGenerator = [[AVAssetImageGenerator alloc] initWithAsset:asset];
    assetImageGenerator.appliesPreferredTrackTransform = YES;
    assetImageGenerator.apertureMode = AVAssetImageGeneratorApertureModeEncodedPixels;
    NSArray *videoTracks = [asset tracksWithMediaType:AVMediaTypeVideo];
    for (AVAssetTrack *track in videoTracks) {
        if (track.naturalSize.width > 0 && track.naturalSize.height > 0) {
            assetImageGenerator.maximumSize = CGSizeMake(track.naturalSize.width, track.naturalSize.height);
        }else{
            assetImageGenerator.maximumSize = CGSizeMake(480, 853);
        }
    }
    CGImageRef thumbnailImageRef = NULL;
    NSError *thumbnailImageGenerationError = nil;
    thumbnailImageRef = [assetImageGenerator copyCGImageAtTime:CMTimeMake(intervalTime, 2) actualTime:NULL error:&thumbnailImageGenerationError];
    
    if (!thumbnailImageRef)
        DLYLog(@"thumbnailImageGenerationError %@", thumbnailImageGenerationError);
    
    UIImage *thumbnailImage = thumbnailImageRef ? [[UIImage alloc] initWithCGImage:thumbnailImageRef] : nil;
    return thumbnailImage;
}

- (UIImage *)imageFromSampleBuffer:(CMSampleBufferRef) sampleBuffer {
    CVImageBufferRef imageBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    CVPixelBufferLockBaseAddress(imageBuffer, 0);
    
    void *baseAddress = CVPixelBufferGetBaseAddress(imageBuffer);
    size_t bytesPerRow = CVPixelBufferGetBytesPerRow(imageBuffer);
    size_t width = CVPixelBufferGetWidth(imageBuffer);
    size_t height = CVPixelBufferGetHeight(imageBuffer);
    
    CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
    
    CGContextRef context = CGBitmapContextCreate(baseAddress, width, height, 8,
                                                 bytesPerRow, colorSpace, kCGBitmapByteOrder32Little | kCGImageAlphaPremultipliedFirst);
    CGImageRef quartzImage = CGBitmapContextCreateImage(context);
    CVPixelBufferUnlockBaseAddress(imageBuffer,0);
    
    CGContextRelease(context);
    CGColorSpaceRelease(colorSpace);
    
    //UIImage *image = [UIImage imageWithCGImage:quartzImage];
    UIImage *image = [UIImage imageWithCGImage:quartzImage scale:1.0f orientation:UIImageOrientationRight];
    
    CGImageRelease(quartzImage);
    
    return (image);
}

#pragma mark - åˆå¹¶ -
- (void) mergeVideoWithVideoTitle:(NSString *)videoTitle SuccessBlock:(SuccessBlock)successBlock failure:(FailureBlock)failureBlcok{
    
    NSMutableArray *videoArray = [NSMutableArray array];
    
    NSFileManager *fileManager = [NSFileManager defaultManager];
    NSString *dataPath = [kPathDocument stringByAppendingPathComponent:kDataFolder];
    if ([[NSFileManager defaultManager] fileExistsAtPath:dataPath]) {
        
        NSString *draftPath = [dataPath stringByAppendingPathComponent:kDraftFolder];
        if ([[NSFileManager defaultManager] fileExistsAtPath:draftPath]) {
            
            NSArray *draftArray = [fileManager contentsOfDirectoryAtPath:draftPath error:nil];
            
            for (NSInteger i = 0; i < [draftArray count]; i++) {
                NSString *path = draftArray[i];
                DLYLog(@"ğŸ”„ğŸ”„ğŸ”„åˆå¹¶ç¬¬ %lu ä¸ªç‰‡æ®µ",i);
                if ([path hasSuffix:@"mov"]) {
                    NSString *allPath = [draftPath stringByAppendingFormat:@"/%@",path];
                    NSURL *url= [NSURL fileURLWithPath:allPath];
                    [videoArray addObject:url];
                }
            }
        }
    }
    DLYLog(@"å¾…åˆæˆçš„è§†é¢‘ç‰‡æ®µ: %@",videoArray);
    AVMutableComposition *mixComposition = [AVMutableComposition composition];
    
    AVMutableCompositionTrack *compositionVideoTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
    AVMutableCompositionTrack *compositionAudioTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];
    
    //æ’å…¥é€šé“çš„æ—¶å€™å¯ä»¥æ”¹å˜è§†é¢‘æ–¹å‘,å¾…æµ‹è¯•ä½¿ç”¨
    //    compositionVideoTrack.preferredTransform = CGAffineTransformRotate(CGAffineTransformIdentity, M_PI_2);
    
    Float64 tmpDuration =0.0f;
    for (int i=0; i < videoArray.count; i++)
    {
        AVURLAsset *videoAsset = [[AVURLAsset alloc]initWithURL:videoArray[i] options:nil];
        
        AVAssetTrack *videoAssetTrack = nil;
        AVAssetTrack *audioAssetTrack = nil;
        if ([videoAsset tracksWithMediaType:AVMediaTypeVideo].count != 0) {
            videoAssetTrack = [[videoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
        }
        if ([videoAsset tracksWithMediaType:AVMediaTypeAudio].count != 0) {
            audioAssetTrack = [[videoAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
        }
        
        CMTimeRange video_timeRange = CMTimeRangeMake(kCMTimeZero,videoAssetTrack.timeRange.duration);
        
        NSError *errorVideo = nil;
        if (videoAssetTrack) {
            [compositionVideoTrack insertTimeRange:video_timeRange ofTrack:videoAssetTrack atTime:CMTimeMakeWithSeconds(tmpDuration, 0) error:&errorVideo];
            if (errorVideo) {
                DLYLog(@"è§†é¢‘åˆæˆè¿‡ç¨‹ä¸­è§†é¢‘è½¨é“æ’å…¥å‘ç”Ÿé”™è¯¯,é”™è¯¯ä¿¡æ¯ :%@",errorVideo);
            }
        }
        
        NSError *errorAudio = nil;
        if (audioAssetTrack) {
            [compositionAudioTrack insertTimeRange:video_timeRange ofTrack:audioAssetTrack atTime:CMTimeMakeWithSeconds(tmpDuration, 0) error:&errorAudio];
            if (errorAudio) {
                DLYLog(@"è§†é¢‘åˆæˆè¿‡ç¨‹éŸ³é¢‘è½¨é“æ’å…¥å‘ç”Ÿé”™è¯¯,é”™è¯¯ä¿¡æ¯ :%@",errorVideo);
            }
        }
        
        tmpDuration += CMTimeGetSeconds(videoAssetTrack.timeRange.duration);
    }
    
    NSURL *productOutputUrl = nil;
    NSString *productPath = [dataPath stringByAppendingPathComponent:kProductFolder];
    if ([[NSFileManager defaultManager] fileExistsAtPath:productPath]) {
        
        _result = [CocoaSecurity md5:[[NSDate date] description]];
        NSString *outputPath = [NSString stringWithFormat:@"%@/%@.mp4",productPath,_result.hex];
        if (outputPath) {
            productOutputUrl = [NSURL fileURLWithPath:outputPath];
        }else{
            DLYLog(@"âŒâŒâŒåˆå¹¶è§†é¢‘ä¿å­˜åœ°å€è·å–å¤±è´¥ !");
        }
    }
    
    AVAssetExportSession *assetExportSession = [[AVAssetExportSession alloc] initWithAsset:mixComposition presetName:AVAssetExportPreset1280x720];
    assetExportSession.outputURL = productOutputUrl;
    assetExportSession.outputFileType = AVFileTypeMPEG4;
    assetExportSession.shouldOptimizeForNetworkUse = YES;
    
    [assetExportSession exportAsynchronouslyWithCompletionHandler:^{
        DLYLog(@"â›³ï¸â›³ï¸â›³ï¸å…¨éƒ¨ç‰‡æ®µmergeæˆåŠŸ");
        DLYMiniVlogTemplate *template = self.session.currentTemplate;
        
        NSString *BGMPath = [[NSBundle mainBundle] pathForResource:template.BGM ofType:@".m4a"];
        NSURL *BGMUrl = [NSURL fileURLWithPath:BGMPath];
        
        [self addMusicToVideo:productOutputUrl audioUrl:BGMUrl videoTitle:videoTitle successBlock:successBlock failure:failureBlcok];
    }];
}
#pragma mark - è½¬åœº -
- (void) addTransitionEffectWithTitle:(NSString *)videoTitle andURL:(NSURL*)newUrl SuccessBlock:(SuccessBlock)successBlock failure:(FailureBlock)failureBlcok{
    
    self.composition = [AVMutableComposition composition];
    
    CMPersistentTrackID trackID = kCMPersistentTrackID_Invalid;
    AVMutableCompositionTrack *compositionTrackA = [self.composition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:trackID];
    AVMutableCompositionTrack *compositionTrackB = [self.composition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:trackID];
    AVMutableCompositionTrack *compositionTrackAudio = [self.composition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:trackID];
    
    //    compositionVideoTrack.preferredTransform = CGAffineTransformRotate(CGAffineTransformIdentity, M_PI_2);

    NSArray *videoTracks = @[compositionTrackA, compositionTrackB];
    
    CMTime videoCursorTime = kCMTimeZero;
    CMTime transitionDuration = CMTimeMake(1, 2);
    CMTime audioCursorTime = kCMTimeZero;
    
    NSMutableArray *videoArray = [NSMutableArray array];
    
    NSFileManager *fileManager = [NSFileManager defaultManager];
    NSString *dataPath = [kPathDocument stringByAppendingPathComponent:kDataFolder];
    if ([[NSFileManager defaultManager] fileExistsAtPath:dataPath]) {
        
        NSString *draftPath = [dataPath stringByAppendingPathComponent:kDraftFolder];
        if ([[NSFileManager defaultManager] fileExistsAtPath:draftPath]) {
            
            NSArray *draftArray = [fileManager contentsOfDirectoryAtPath:draftPath error:nil];
            
            for (NSInteger i = 0; i < [draftArray count]; i++) {
                NSString *path = draftArray[i];
                DLYLog(@"ğŸ”„ğŸ”„ğŸ”„åˆå¹¶-->åŠ è½½--> ç¬¬ %lu ä¸ªç‰‡æ®µ",i);
                if ([path hasSuffix:@"mp4"]) {
                    NSString *allPath = [draftPath stringByAppendingFormat:@"/%@",path];
                    NSURL *url= [NSURL fileURLWithPath:allPath];
                    [videoArray addObject:url];
                }
            }
        }
    }
    DLYLog(@"å¾…åˆæˆçš„è§†é¢‘ç‰‡æ®µ: %@",videoArray);
    
    for (NSUInteger i = 0; i < videoArray.count; i++) {
        
        NSUInteger trackIndex = i % 2;
        
        AVURLAsset *asset;
//        if (i == 0) {
//            asset = [AVURLAsset URLAssetWithURL:newUrl options:nil];
//        }else {
            asset = [AVURLAsset URLAssetWithURL:videoArray[i] options:nil];
//        }
        
        AVAssetTrack *assetVideoTrack = nil;
        AVAssetTrack *assetAudioTrack = nil;
        if ([asset tracksWithMediaType:AVMediaTypeVideo].count != 0) {
            assetVideoTrack = [[asset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
        }
        if ([asset tracksWithMediaType:AVMediaTypeAudio].count != 0) {
            assetAudioTrack = [[asset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
        }
        AVMutableCompositionTrack *currentTrack = videoTracks[trackIndex];
        
        CMTimeRange timeRange = CMTimeRangeMake(kCMTimeZero, assetVideoTrack.timeRange.duration);
        
        BOOL isInsertVideoSuccess = [currentTrack insertTimeRange:timeRange
                                                          ofTrack:assetVideoTrack
                                                           atTime:videoCursorTime error:nil];
        if (isInsertVideoSuccess == NO) {
            DLYLog(@"åˆå¹¶æ—¶æ’å…¥å›¾åƒè½¨å¤±è´¥");
        }
        BOOL isInsertAudioSuccess = [compositionTrackAudio insertTimeRange:timeRange
                                                                   ofTrack:assetAudioTrack
                                                                    atTime:videoCursorTime error:nil];
        if (isInsertAudioSuccess == NO) {
            DLYLog(@"åˆå¹¶æ—¶æ’å…¥éŸ³è½¨å¤±è´¥");
        }
        
        videoCursorTime = CMTimeAdd(videoCursorTime, timeRange.duration);
        videoCursorTime = CMTimeSubtract(videoCursorTime, transitionDuration);
//        audioCursorTime = CMTimeAdd(audioCursorTime, timeRange.duration);
        
        if (i + 1 < videoArray.count) {
            timeRange = CMTimeRangeMake(videoCursorTime, transitionDuration);
            NSValue *timeRangeValue = [NSValue valueWithCMTimeRange:timeRange];
            [self.transitionTimeRanges addObject:timeRangeValue];
        }
    }
    
    AVVideoComposition *videoComposition = [self buildVideoComposition];
    
    NSURL *productOutputUrl = nil;
    NSString *productPath = [dataPath stringByAppendingPathComponent:kProductFolder];
    if ([[NSFileManager defaultManager] fileExistsAtPath:productPath]) {
        
        _result = [CocoaSecurity md5:[[NSDate date] description]];
        NSString *outputPath = [NSString stringWithFormat:@"%@/%@.mp4",productPath,_result.hex];
        if (outputPath) {
            productOutputUrl = [NSURL fileURLWithPath:outputPath];
        }else{
            DLYLog(@"âŒâŒâŒåˆå¹¶è§†é¢‘ä¿å­˜åœ°å€è·å–å¤±è´¥ !");
        }
    }
    
    AVAssetExportSession *assetExportSession = [[AVAssetExportSession alloc] initWithAsset:self.composition presetName:AVAssetExportPreset1280x720];
    assetExportSession.videoComposition = videoComposition;
    assetExportSession.outputURL = productOutputUrl;
    assetExportSession.outputFileType = AVFileTypeMPEG4;
    assetExportSession.shouldOptimizeForNetworkUse = YES;
    
    [assetExportSession exportAsynchronouslyWithCompletionHandler:^{
        DLYLog(@"â›³ï¸â›³ï¸â›³ï¸å…¨éƒ¨ç‰‡æ®µmergeæˆåŠŸ");
        DLYMiniVlogTemplate *template = self.session.currentTemplate;
        
        NSString *BGMPath = [[NSBundle mainBundle] pathForResource:template.BGM ofType:@".m4a"];
        NSURL *BGMUrl = [NSURL fileURLWithPath:BGMPath];
        
        [self addVideoFilter:productOutputUrl audioUrl:BGMUrl videoTitle:videoTitle];
//        [self addMusicToVideo:productOutputUrl audioUrl:BGMUrl videoTitle:videoTitle successBlock:successBlock failure:failureBlcok];
    }];
}
- (AVVideoComposition *)buildVideoComposition {
    
    AVVideoComposition *videoComposition = [AVMutableVideoComposition videoCompositionWithPropertiesOfAsset:self.composition];
    
    NSArray *transitionInstructions = [self transitionInstructionsInVideoComposition:videoComposition];
    
    for (DLYTransitionInstructions *instructions in transitionInstructions) {
        
        CMTimeRange timeRange = instructions.compositionInstruction.timeRange;
        
        AVMutableVideoCompositionLayerInstruction *fromLayer = instructions.fromLayerInstruction;
        
        AVMutableVideoCompositionLayerInstruction *toLayer = instructions.toLayerInstruction;
        
        CGAffineTransform identityTransform = CGAffineTransformIdentity;
        
        CGFloat videoWidth = videoComposition.renderSize.width;
        CGFloat videoHeight = videoComposition.renderSize.height;
        NSLog(@"videoWidth: %f,videoHeight: %f",videoWidth,videoHeight);
        //Transform
        CGAffineTransform fromDestTransform = CGAffineTransformMakeTranslation(-videoWidth, 0.0);
        CGAffineTransform toStartTransform = CGAffineTransformMakeTranslation(videoWidth, 0.0);
        
        CGAffineTransform transform1 = CGAffineTransformMakeRotation(M_PI);
        CGAffineTransform transform2 = CGAffineTransformScale(transform1, 2.0, 2.0);
        
        //Rotation
        CGAffineTransform fromDestTransformRotation = CGAffineTransformMakeRotation(-M_PI);
        CGAffineTransform toStartTransformRotation = CGAffineTransformMakeRotation(M_PI);
        
        //ç¼©æ”¾
        CGAffineTransform fromTransformScale = CGAffineTransformMakeScale(2, 2);
        CGAffineTransform toTransformScale = CGAffineTransformMakeScale(2, 2);
        
        DLYVideoTransitionType type = instructions.transition.type;
        
        switch (type) {
            case DLYVideoTransitionTypeDissolve:
                
                [fromLayer setOpacityRampFromStartOpacity:1.0 toEndOpacity:0.0 timeRange:timeRange];
                break;
            case DLYVideoTransitionTypePush:
                
                [fromLayer setTransformRampFromStartTransform:identityTransform
                                               toEndTransform:fromDestTransform
                                                    timeRange:timeRange];
                
                [toLayer setTransformRampFromStartTransform:toStartTransform
                                             toEndTransform:identityTransform
                                                  timeRange:timeRange];
                break;
            case DLYVideoTransitionTypeWipe:
                
                [fromLayer setTransformRampFromStartTransform:identityTransform
                                               toEndTransform:transform2
                                                    timeRange:timeRange];
                
                [toLayer setTransformRampFromStartTransform:transform2
                                             toEndTransform:identityTransform
                                                  timeRange:timeRange];
                break;
            case DLYVideoTransitionTypeClockwiseRotate:
                
                [fromLayer setTransformRampFromStartTransform:identityTransform
                                               toEndTransform:fromDestTransformRotation
                                                    timeRange:timeRange];
                
                [toLayer setTransformRampFromStartTransform:toStartTransformRotation
                                             toEndTransform:identityTransform
                                                  timeRange:timeRange];
                break;
            case DLYVideoTransitionTypeZoom:
                
                [fromLayer setTransformRampFromStartTransform:identityTransform toEndTransform:fromTransformScale timeRange:timeRange];
                [toLayer setTransformRampFromStartTransform:identityTransform toEndTransform:toTransformScale timeRange:timeRange];
                
                break;
                
            default:
                break;
        }
        
        instructions.compositionInstruction.layerInstructions = @[fromLayer,toLayer];
    }
    return videoComposition;
}
- (NSArray *)transitionInstructionsInVideoComposition:(AVVideoComposition *)vc {
    
    NSMutableArray *transitionInstructions = [NSMutableArray array];
    
    int layerInstructionIndex = 1;
    
    NSArray *compositionInstructions = vc.instructions;
    
    for (AVMutableVideoCompositionInstruction *vci in compositionInstructions) {
        
        if (vci.layerInstructions.count == 2) {
            
            DLYTransitionInstructions *instructions = [[DLYTransitionInstructions alloc] init];
            
            instructions.compositionInstruction = vci;
            
            instructions.fromLayerInstruction =
            (AVMutableVideoCompositionLayerInstruction *)vci.layerInstructions[1 - layerInstructionIndex];
            
            instructions.toLayerInstruction =
            (AVMutableVideoCompositionLayerInstruction *)vci.layerInstructions[layerInstructionIndex];
            
            [transitionInstructions addObject:instructions];
            
            layerInstructionIndex = layerInstructionIndex == 1 ? 0 : 1;
        }
    }
    
    for (NSUInteger i = 0; i < transitionInstructions.count; i++) {
        
        DLYMiniVlogTemplate *template = self.session.currentTemplate;
        DLYMiniVlogPart *part = template.parts[i];
        DLYVideoTransitionType transitionType = part.transitionType;
        
        DLYTransitionInstructions *tis = transitionInstructions[i];
        
        DLYVideoTransition *transition = [DLYVideoTransition videoTransition];
        
        if (transitionType == DLYVideoTransitionTypeNone) {
            
        }else{
            transition.type = transitionType;
            tis.transition = transition;
        }
    }
    return transitionInstructions;
}
#pragma mark - é…éŸ³ -
- (void) addMusicToVideo:(NSURL *)videoUrl audioUrl:(NSURL *)audioUrl videoTitle:(NSString *)videoTitle successBlock:(SuccessBlock)successBlock failure:(FailureBlock)failureBlcok{
    
    AVURLAsset *videoAsset = [AVURLAsset URLAssetWithURL:videoUrl options:nil];
    AVURLAsset *audioAsset = [AVURLAsset URLAssetWithURL:audioUrl options:nil];
    
    AVAssetTrack *videoAssetTrack = nil;
    AVAssetTrack *audioAssetTrack = nil;
    
    if ([[videoAsset tracksWithMediaType:AVMediaTypeVideo] count] != 0) {
        videoAssetTrack = [[videoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
    }
    if ([[audioAsset tracksWithMediaType:AVMediaTypeAudio] count] != 0) {
        audioAssetTrack = [[audioAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
    }
    AVMutableComposition* mixComposition = [AVMutableComposition composition];
    
    CMPersistentTrackID trackID = kCMPersistentTrackID_Invalid;
    AVMutableCompositionTrack *videoCompositionTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:trackID];
    AVMutableCompositionTrack *audioCompositionTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:trackID];
    
    NSError *error = nil;
    if (videoAssetTrack) {
        [videoCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, videoAssetTrack.timeRange.duration) ofTrack:videoAssetTrack atTime:kCMTimeZero error:&error];
    }
    if (audioAssetTrack) {
        [audioCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, audioAssetTrack.timeRange.duration) ofTrack:audioAssetTrack atTime:kCMTimeZero error:&error];
    }
    
    [videoCompositionTrack setPreferredTransform:videoAssetTrack.preferredTransform];
    
    //æ·»åŠ æ ‡é¢˜
    AVMutableVideoComposition *mutableVideoComposition = [AVMutableVideoComposition videoComposition];
    
    if ([[mixComposition tracksWithMediaType:AVMediaTypeVideo] count] != 0) {
        // build a pass through video composition
        mutableVideoComposition.frameDuration = CMTimeMake(1, 30);
        mutableVideoComposition.renderSize = videoAssetTrack.naturalSize;
        
        AVMutableVideoCompositionInstruction *passThroughInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
        passThroughInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, [mixComposition duration]);
        
        AVAssetTrack *videoTrack = [mixComposition tracksWithMediaType:AVMediaTypeVideo][0];
        AVMutableVideoCompositionLayerInstruction *passThroughLayer = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoTrack];
        
        passThroughInstruction.layerInstructions = @[passThroughLayer];
        mutableVideoComposition.instructions = @[passThroughInstruction];
        
        CGSize renderSize = mutableVideoComposition.renderSize;
        CALayer *videoTitleLayer = [self addTitleForVideoWith:videoTitle size:renderSize];
        
        CALayer *parentLayer = [CALayer layer];
        CALayer *videoLayer = [CALayer layer];
        parentLayer.frame = CGRectMake(0, 0, mutableVideoComposition.renderSize.width, mutableVideoComposition.renderSize.height);
        videoLayer.frame = CGRectMake(0, 0, mutableVideoComposition.renderSize.width, mutableVideoComposition.renderSize.height);
        [parentLayer addSublayer:videoLayer];
        
        videoTitleLayer.position = CGPointMake(mutableVideoComposition.renderSize.width / 2, mutableVideoComposition.renderSize.height / 2);
        [parentLayer addSublayer:videoTitleLayer];
        
        if (APPTEST) {
            CALayer *watermarkLayer = [CALayer layer];
            watermarkLayer = [self addWatermarkWithSize:renderSize];
            watermarkLayer.position = CGPointMake(mutableVideoComposition.renderSize.width - 358, 8);
            [parentLayer addSublayer:watermarkLayer];
        }
        
        mutableVideoComposition.animationTool = [AVVideoCompositionCoreAnimationTool videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayer:videoLayer inLayer:parentLayer];
    }
    
    //å¤„ç†è§†é¢‘åŸå£°
    AVAssetTrack *originalAudioAssetTrack = nil;
    if ([[videoAsset tracksWithMediaType:AVMediaTypeAudio] count] != 0) {
        originalAudioAssetTrack = [[videoAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
    }
    
    AVMutableCompositionTrack *originalAudioCompositionTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];
    
    [originalAudioCompositionTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, videoAssetTrack.timeRange.duration) ofTrack:originalAudioAssetTrack atTime:kCMTimeZero error:nil];
    
    AVMutableAudioMix *audioMix = [AVMutableAudioMix audioMix];
    
    AVMutableAudioMixInputParameters *videoParameters = [AVMutableAudioMixInputParameters audioMixInputParametersWithTrack:originalAudioCompositionTrack];
    AVMutableAudioMixInputParameters *BGMParameters = [AVMutableAudioMixInputParameters audioMixInputParametersWithTrack:audioCompositionTrack];
    
    NSArray *partArray = self.session.currentTemplate.parts;
    
    for (NSInteger i = 0; i < partArray.count; i++) {
        
        DLYMiniVlogPart *part = partArray[i];
        
        NSArray *startArr = [part.dubStartTime componentsSeparatedByString:@":"];
        NSString *startTimeStr = startArr[1];
        float startTime = [startTimeStr floatValue];
        _startTime = CMTimeMake(startTime, 1);
        
        NSArray *stopArr = [part.dubStopTime componentsSeparatedByString:@":"];
        NSString *stopTimeStr = stopArr[1];
        float stopTime = [stopTimeStr floatValue];
        _stopTime = CMTimeMake(stopTime, 1);
        
        //æ—¶é•¿å°äº1sçš„ç‰‡æ®µéŸ³è½¨åˆ‡æ¢å¹³æ»‘ç‰¹æ®Šå¤„ç†
        float rampOffsetValue = 1;
        
        _prePoint = CMTimeMake(stopTime - rampOffsetValue, 1);
        CMTime duration = CMTimeSubtract(_stopTime, _prePoint);
        
        CMTimeRange timeRange = CMTimeRangeMake(_startTime, duration);
        CMTimeRange preTimeRange = CMTimeRangeMake(_prePoint, CMTimeMake(2, 1));
        
        if (part.soundType == DLYMiniVlogAudioTypeMusic) {//ç©ºé•œ
            [BGMParameters setVolumeRampFromStartVolume:part.BGMVolume / 100 toEndVolume:part.BGMVolume / 100 timeRange:timeRange];
            //            [BGMParameters setVolumeRampFromStartVolume:5.0 toEndVolume:0.4 timeRange:preTimeRange];
            
            [videoParameters setVolumeRampFromStartVolume:0 toEndVolume:0 timeRange:timeRange];
        }else if(part.soundType == DLYMiniVlogAudioTypeNarrate){//äººå£°
            [videoParameters setVolumeRampFromStartVolume:2.0 toEndVolume:2.0 timeRange:timeRange];
            [BGMParameters setVolumeRampFromStartVolume:part.BGMVolume / 100 toEndVolume:part.BGMVolume / 100 timeRange:timeRange];
            //            [BGMParameters setVolumeRampFromStartVolume:0.4 toEndVolume:5.0 timeRange:preTimeRange];
        }
    }
    audioMix.inputParameters = @[videoParameters,BGMParameters];
    
    NSURL *outPutUrl = [self.resource saveProductToSandbox];
    self.currentProductUrl = outPutUrl;
    
    if ([[NSFileManager defaultManager] fileExistsAtPath:self.currentProductUrl.absoluteString])
    {
        [[NSFileManager defaultManager] removeItemAtPath:self.currentProductUrl.absoluteString error:nil];
    }
    
    AVAssetExportSession *assetExportSession = [[AVAssetExportSession alloc] initWithAsset:mixComposition presetName:AVAssetExportPreset1280x720];
    assetExportSession.outputURL = outPutUrl;
    assetExportSession.audioMix = audioMix;
    assetExportSession.videoComposition = mutableVideoComposition;
    assetExportSession.outputFileType = AVFileTypeMPEG4;
    assetExportSession.shouldOptimizeForNetworkUse = YES;
    
    [assetExportSession exportAsynchronouslyWithCompletionHandler:^{
        switch ([assetExportSession status]) {
            case AVAssetExportSessionStatusFailed:{
                DLYLog(@"é…éŸ³å¤±è´¥: %@",[[assetExportSession error] description]);
            }break;
            case AVAssetExportSessionStatusCompleted:{
                successBlock();
                if ([self.delegate  respondsToSelector:@selector(didFinishEdititProductUrl:)]) {
                    [self.delegate didFinishEdititProductUrl:outPutUrl];
                }
                ALAssetsLibrary *assetLibrary = [[ALAssetsLibrary alloc] init];
                [assetLibrary saveVideo:outPutUrl toAlbum:@"ä¸€åˆ†" completionBlock:^(NSURL *assetURL, NSError *error) {
                    DLYLog(@"â›³ï¸â›³ï¸â›³ï¸é…éŸ³å®Œæˆåä¿å­˜åœ¨æ‰‹æœºç›¸å†Œ");
                    BOOL isSuccess = NO;
                    NSFileManager *fileManager = [NSFileManager defaultManager];
                    
                    NSString *dataPath = [kPathDocument stringByAppendingPathComponent:kDataFolder];
                    NSString *productPath = [dataPath stringByAppendingPathComponent:kProductFolder];
                    
                    if ([[NSFileManager defaultManager] fileExistsAtPath:productPath]) {
                        
                        NSString *targetPath = [productPath stringByAppendingFormat:@"/%@.mp4",_result.hex];
                        isSuccess = [fileManager removeItemAtPath:targetPath error:nil];
                        DLYLog(@"%@",isSuccess ? @"â›³ï¸â›³ï¸â›³ï¸æˆåŠŸåˆ é™¤æœªé…éŸ³çš„æˆç‰‡è§†é¢‘ !" : @"âŒâŒâŒåˆ é™¤æœªé…éŸ³è§†é¢‘å¤±è´¥");
                    }
                    NSString *outPath1 = @"outputMovie1.mp4";
                    NSString *tempoutPath1 = [NSTemporaryDirectory() stringByAppendingPathComponent:outPath1];
                    if ([[NSFileManager defaultManager] fileExistsAtPath:tempoutPath1]) {
                        isSuccess = [fileManager removeItemAtPath:tempoutPath1 error:nil];
                        DLYLog(@"åˆ é™¤æ»¤é•œ");
                    }
                    NSString *outPath2 = @"outputMovie2.mp4";
                    NSString *tempoutPath2 = [NSTemporaryDirectory() stringByAppendingPathComponent:outPath2];
                    if ([[NSFileManager defaultManager] fileExistsAtPath:tempoutPath2]) {
                        isSuccess = [fileManager removeItemAtPath:tempoutPath2 error:nil];
                        DLYLog(@"åˆ é™¤ç‰‡å¤´ç‰‡å°¾");
                    }
                } failureBlock:^(NSError *error) {
                }];
            }break;
            default:
                break;
        }
    }];
}
#pragma mark - æ·»åŠ æµ‹è¯•æ°´å° -
- (CALayer *) addWatermarkWithSize:(CGSize)renderSize
{
    CALayer *overlayLayer = [CALayer layer];
    CATextLayer *watermarkLayer = [CATextLayer layer];
    UIFont *font = [UIFont systemFontOfSize:24.0];
    
    //è·å–å½“å‰æ—¶é—´
    NSString *currentTime  = [self getCurrentTime];
    //è·å–å½“å‰ç‰ˆæœ¬å·
    NSDictionary*infoDic = [[NSBundle mainBundle] infoDictionary];
    NSString *localVersion = [infoDic objectForKey:@"CFBundleShortVersionString"];
    //è·å–å½“å‰buildå·
    NSString *buildVersion = [infoDic objectForKey:@"CFBundleVersion"];
    //è·å–ç³»ç»Ÿç‰ˆæœ¬
    NSString *currentSystemVersion = [[UIDevice currentDevice] systemVersion];
    //è·å–æœºå‹
    DLYMobileDevice *mobileDevice = [DLYMobileDevice sharedDevice];
    _currentDeviceType = [mobileDevice iPhoneModel];
    
    NSString *watermarkMessage = [self.session.currentTemplate.templateTitle stringByAppendingFormat:@"   %@  %@  %@(%@)   %@",_currentDeviceType,currentSystemVersion,localVersion,buildVersion,currentTime];
    
    [watermarkLayer setFontSize:24.f];
    [watermarkLayer setFont:@"ArialRoundedMTBold"];
    [watermarkLayer setString:watermarkMessage];
    [watermarkLayer setAlignmentMode:kCAAlignmentCenter];
    [watermarkLayer setForegroundColor:[[UIColor colorWithHexString:@"FFFFFF" withAlpha:1] CGColor]];
    [watermarkLayer setBackgroundColor:[[UIColor colorWithHexString:@"#000000" withAlpha:0.8] CGColor]];
    watermarkLayer.contentsCenter = overlayLayer.contentsCenter;
    CGSize textSize = [watermarkMessage sizeWithAttributes:[NSDictionary dictionaryWithObjectsAndKeys:font,NSFontAttributeName, nil]];
    watermarkLayer.bounds = CGRectMake(0, 0, textSize.width + 50, textSize.height + 25);
    
    [overlayLayer addSublayer:watermarkLayer];
    return overlayLayer;
}
#pragma mark - è§†é¢‘æ ‡é¢˜è®¾ç½® -
- (CALayer *) addTitleForVideoWith:(NSString *)titleText size:(CGSize)renderSize{
    
    CALayer *overlayLayer = [CALayer layer];
    CATextLayer *titleLayer = [CATextLayer layer];
    UIFont *font = [UIFont systemFontOfSize:68.0];
    
    [titleLayer setFontSize:68.f];
    [titleLayer setFont:@"LingWaiSCMedium"];//HanziPenTCRegular/LingWaiSC
    [titleLayer setString:titleText];
    [titleLayer setAlignmentMode:kCAAlignmentCenter];
    [titleLayer setForegroundColor:[[UIColor colorWithHexString:@"#FFD700" withAlpha:0.8] CGColor]];
    titleLayer.contentsCenter = overlayLayer.contentsCenter;
    CGSize textSize = [titleText sizeWithAttributes:[NSDictionary dictionaryWithObjectsAndKeys:font,NSFontAttributeName, nil]];
    titleLayer.bounds = CGRectMake(0, 0, textSize.width + 50, textSize.height + 25);
    
    DLYMiniVlogTemplate *template = self.session.currentTemplate;
    NSDictionary *subTitleDic = template.subTitle1;
    NSString *subTitleStart = [subTitleDic objectForKey:@"startTime"];
    NSString *subTitleStop = [subTitleDic objectForKey:@"stopTime"];
    
    float _subTitleStart = [self switchTimeWithTemplateString:subTitleStart] / 1000;
    float _subTitleStop = [self switchTimeWithTemplateString:subTitleStop] / 1000;
    float duration = _subTitleStop - _subTitleStart;
    
    CABasicAnimation *animation1 = [CABasicAnimation animationWithKeyPath:@"opacity"];
    animation1.fromValue = [NSNumber numberWithFloat:0.0f];
    animation1.toValue = [NSNumber numberWithFloat:0.0f];
    animation1.repeatCount = 0;
    animation1.duration = _subTitleStart;
    [animation1 setRemovedOnCompletion:NO];
    [animation1 setFillMode:kCAFillModeForwards];
    animation1.beginTime = AVCoreAnimationBeginTimeAtZero;
    [titleLayer addAnimation:animation1 forKey:@"opacityAniamtion"];
    
    CABasicAnimation *animation2 = [CABasicAnimation animationWithKeyPath:@"opacity"];
    animation2.fromValue = [NSNumber numberWithFloat:1.0f];
    animation2.toValue = [NSNumber numberWithFloat:0.0f];
    animation2.repeatCount = 0;
    animation2.duration = duration;
    [animation2 setRemovedOnCompletion:NO];
    [animation2 setFillMode:kCAFillModeForwards];
    animation2.beginTime = _subTitleStart;
    [titleLayer addAnimation:animation2 forKey:@"opacityAniamtion1"];
    
    [overlayLayer addSublayer:titleLayer];
    
    return overlayLayer;
}

#pragma mark - è§†é¢‘å åŠ  -
- (void) overlayVideoForBodyVideoAction{
    
    NSURL *alphaUrl = [[NSBundle mainBundle] URLForResource:@"testheadergreenh264" withExtension:@"mp4"];
    NSURL *bodyUrl = [[NSBundle mainBundle] URLForResource:@"01_nebula" withExtension:@"mp4"];
    
    AVURLAsset *bodyAsset = [AVURLAsset URLAssetWithURL:bodyUrl options:nil];
    AVAssetTrack *videoTrack = [[bodyAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
    _videoSize = videoTrack.naturalSize;
    
    self.bodyMovie = [[GPUImageMovie alloc]initWithURL:bodyUrl];
    self.alphaMovie = [[GPUImageMovie alloc]initWithURL:alphaUrl];
    
    self.filter = [[GPUImageChromaKeyBlendFilter alloc] init];
    
    [self.alphaMovie addTarget:self.filter];
    [self.bodyMovie addTarget:self.filter];
    
    NSURL *outputUrl = [self.resource saveToSandboxFolderType:NSDocumentDirectory subfolderName:@"HeaderVideos" suffixType:@".mp4"];
    self.movieWriter =  [[GPUImageMovieWriter alloc] initWithMovieURL:outputUrl size:_videoSize];
    
    [self.filter addTarget:self.movieWriter];
    
    [self.movieWriter startRecording];
    [self.bodyMovie startProcessing];
    [self.alphaMovie startProcessing];
    
    __weak typeof(self) weakSelf = self;
    
    [self.movieWriter setCompletionBlock:^{
        
        [weakSelf.alphaMovie endProcessing];
        [weakSelf.bodyMovie endProcessing];
        [weakSelf.movieWriter finishRecording];
        
        dispatch_async(dispatch_get_main_queue(), ^{
            
            // ä¿å­˜åˆ°ç›¸å†Œ
            //            [weakSelf writeToAlbum:outputUrl];
        });
    }];
}
#pragma mark - åª’ä½“æ–‡ä»¶æˆªå– -
-(void)trimVideoByRange:(NSURL *)assetUrl startTime:(CMTime)startTime stop:(CMTime)stopTime{
    
    AVAsset *selectedAsset = [AVAsset assetWithURL:assetUrl];
    AVAssetTrack *videoAssertTrack = nil;
    AVAssetTrack *audioAssertTrack = nil;
    
    if ([[selectedAsset tracksWithMediaType:AVMediaTypeVideo]objectAtIndex:0]) {
        videoAssertTrack = [[selectedAsset tracksWithMediaType:AVMediaTypeVideo]objectAtIndex:0];
    }
    if ([[selectedAsset tracksWithMediaType:AVMediaTypeAudio]objectAtIndex:0]) {
        audioAssertTrack = [[selectedAsset tracksWithMediaType:AVMediaTypeAudio]objectAtIndex:0];
    }
    
    AVMutableComposition *composition = [AVMutableComposition composition];
    
    CMTimeRange videoTimeRange = CMTimeRangeMake(startTime,stopTime);
    
    AVMutableCompositionTrack *videoCompositionTrack = [composition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
    AVMutableCompositionTrack *audioCompositionTrack = [composition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];
    [videoCompositionTrack insertTimeRange:videoTimeRange ofTrack:videoAssertTrack atTime:kCMTimeZero error:nil];
    [audioCompositionTrack insertTimeRange:videoTimeRange ofTrack:audioAssertTrack atTime:kCMTimeZero error:nil];
    
    AVMutableVideoCompositionLayerInstruction *videoCompositionLayerIns = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoAssertTrack];
    [videoCompositionLayerIns setTransform:videoAssertTrack.preferredTransform atTime:kCMTimeZero];
    
    AVMutableVideoCompositionInstruction *videoCompositionIns = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
    [videoCompositionIns setTimeRange:CMTimeRangeMake(kCMTimeZero, videoAssertTrack.timeRange.duration)];

    AVMutableVideoComposition *videoComposition = [AVMutableVideoComposition videoComposition];
    videoComposition.instructions = @[videoCompositionIns];
    videoComposition.renderSize = CGSizeMake(videoAssertTrack.naturalSize.height,videoAssertTrack.naturalSize.width);

    videoComposition.frameDuration = CMTimeMake(1, 60);
    
    AVMutableVideoCompositionLayerInstruction *layerInst;
    layerInst = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:videoAssertTrack];
    [layerInst setTransform:videoAssertTrack.preferredTransform atTime:kCMTimeZero];
    AVMutableVideoCompositionInstruction *inst = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
    inst.timeRange = CMTimeRangeMake(kCMTimeZero, selectedAsset.duration);
    inst.layerInstructions = [NSArray arrayWithObject:layerInst];
    videoComposition.instructions = [NSArray arrayWithObject:inst];
}
#pragma mark - æ—¶é—´å¤„ç† -
- (float) switchTimeWithTemplateString:(NSString *)timeSting{
    
    float timePoint = 0;
    NSArray *startArr = [timeSting componentsSeparatedByString:@":"];
    
    for (int i = 0; i < 3; i ++) {
        NSString *timeStr = startArr[i];
        int time = [timeStr floatValue];
        if (i == 0) {
            timePoint = timePoint + time * 60 * 1000;
        }if (i == 1) {
            timePoint = timePoint + time * 1000;
        }else {
            timePoint = timePoint + time;
        }
    }
    return timePoint;
}

-(long long)getDateTimeTOMilliSeconds:(NSDate *)datetime {
    NSTimeInterval interval = [datetime timeIntervalSince1970];
    long long totalMilliseconds = interval * 1000;
    return totalMilliseconds;
}
//è·å–å½“åœ°æ—¶é—´
- (NSString *)getCurrentTime {
    NSDateFormatter *formatter = [[NSDateFormatter alloc] init];
    [formatter setDateFormat:@"yyyy.MM.dd  HH:mm:ss"];
    NSString *dateTime = [formatter stringFromDate:[NSDate date]];
    return dateTime;
}
#pragma mark - åŠ æ»¤é•œ
- (void)addVideoFilter:(NSURL *)videoUrl audioUrl:BGMUrl videoTitle:videoTitle {
    AVURLAsset* asset = [AVURLAsset assetWithURL:videoUrl];
    AVAssetTrack *asetTrack = [[asset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
    NSString *inputpath = @"outputMovie1.mp4";
    NSString* tempVideoPath = [NSTemporaryDirectory() stringByAppendingPathComponent:inputpath];
    unlink([tempVideoPath UTF8String]);
    NSURL *tempVideo = [NSURL fileURLWithPath:tempVideoPath];
    //1. ä¼ å…¥è§†é¢‘æ–‡ä»¶
    //    _movieFile = [[GPUImageMovie alloc] initWithURL:videoUrl];
    
    //2. æ·»åŠ æ»¤é•œ
    [self initializeVideo:videoUrl];
    CGSize videoSize = CGSizeMake(asetTrack.naturalSize.width, asetTrack.naturalSize.height);
    
    // 3.
    _inputMovieWriter = [[GPUImageMovieWriter alloc] initWithMovieURL:tempVideo size:videoSize];
    if ((NSNull*)_outputFilter != [NSNull null] && _outputFilter != nil)
    {
        [_outputFilter addTarget:_inputMovieWriter];
    }
    else
    {
        [_movieFile addTarget:_inputMovieWriter];
    }
    
    // 4. Configure this for video from the movie file, where we want to preserve all video frames and audio samples
    _inputMovieWriter.shouldPassthroughAudio = YES;
    _movieFile.audioEncodingTarget = _inputMovieWriter;
    [_movieFile enableSynchronizedEncodingUsingMovieWriter:_inputMovieWriter];
    
    // 5.
    [_inputMovieWriter startRecording];
    [_movieFile startProcessing];
    __unsafe_unretained typeof(self) weakSelf = self;
    // 7. Filter effect finished
    [weakSelf.inputMovieWriter setCompletionBlock:^{
        
        if ((NSNull*)_outputFilter != [NSNull null] && _outputFilter != nil)
        {
            [_outputFilter removeTarget:weakSelf.inputMovieWriter];
        }
        else
        {
            [_movieFile removeTarget:weakSelf.inputMovieWriter];
        }
        
        [_inputMovieWriter finishRecordingWithCompletionHandler:^{
            // å®Œæˆåå¤„ç†è¿›åº¦è®¡æ—¶å™¨ å…³é—­ã€æ¸…ç©º
            NSLog(@"å®Œæˆ");
            NSString *outPath = @"outputMovie2.mp4";
            NSString *tempoutPath = [NSTemporaryDirectory() stringByAppendingPathComponent:outPath];
            NSMutableArray *arr = [NSMutableArray array];
            [self buildVideoEffectsToMP4:tempoutPath inputVideoURL:tempVideo andImageArray:arr callback:^(NSURL *finalUrl, NSString *filePath) {
                //åŠ å…¥èƒŒæ™¯éŸ³ä¹
                [self addMusicToVideo:finalUrl audioUrl:BGMUrl videoTitle:videoTitle successBlock:^{
                    NSLog(@"é…éŸ³æˆåŠŸ");
                } failure:^(NSError *error) {
                    NSLog(@"");
                }];
            }];
        }];
        
    }];
}
- (void)initializeVideo:(NSURL*) inputMovieURL {
    // 1.
    _movieFile = [[GPUImageMovie alloc] initWithURL:inputMovieURL];
    _movieFile.runBenchmark = NO;
    _movieFile.playAtActualSpeed = NO;
    
    // 2. Add filter effect
    _outputFilter = nil;
    _outputFilter = [self addVideoFilter:_movieFile];
}
- (GPUImageOutput<GPUImageInput> *)addVideoFilter:(GPUImageMovie *)movieFile {
    GPUImageOutput<GPUImageInput> *filterCurrent;
    
    GPUImageFilter *filt = [[GPUImageFilter alloc]init];
    filterCurrent = filt;
    [movieFile addTarget:filterCurrent];

//    DLYVideoFilter *filt = [[DLYVideoFilter alloc]init];
//    filterCurrent = filt;
//    [movieFile addTarget:filt];

    return filterCurrent;
}
#pragma mark - åŠ¨æ€æ°´å°
- (BOOL)buildVideoEffectsToMP4:(NSString *)exportVideoFile inputVideoURL:(NSURL *)inputVideoURL andImageArray:(NSMutableArray *)imageArr callback:(Callback )callBlock{
    
    // 1.
    if (!inputVideoURL || ![inputVideoURL isFileURL] || !exportVideoFile || [exportVideoFile isEqualToString:@""]) {
        NSLog(@"Input filename or Output filename is invalied for convert to Mp4!");
        return NO;
    }
    
    unlink([exportVideoFile UTF8String]);
    
    // 2. Create the composition and tracks
    AVURLAsset *asset = [[AVURLAsset alloc] initWithURL:inputVideoURL options:nil];
    NSParameterAssert(asset);
    if(asset ==nil || [[asset tracksWithMediaType:AVMediaTypeVideo] count]<1) {
        NSLog(@"Input video is invalid!");
        return NO;
    }
    
    AVMutableComposition *composition = [AVMutableComposition composition];
    AVMutableCompositionTrack *videoTrack = [composition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
    AVMutableCompositionTrack *audioTrack = [composition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];
    
    NSArray *assetVideoTracks = [asset tracksWithMediaType:AVMediaTypeVideo];
    if (assetVideoTracks.count <= 0)
    {
        // Retry once
        if (asset)
        {
            asset = nil;
        }
        
        asset = [[AVURLAsset alloc] initWithURL:inputVideoURL options:nil];
        assetVideoTracks = [asset tracksWithMediaType:AVMediaTypeVideo];
        if ([assetVideoTracks count] <= 0)
        {
            if (asset)
            {
                asset = nil;
            }
            
            NSLog(@"Error reading the transformed video track");
            return NO;
        }
    }
    
    // 3. Insert the tracks in the composition's tracks
    AVAssetTrack *assetVideoTrack = [assetVideoTracks firstObject];
    [videoTrack insertTimeRange:assetVideoTrack.timeRange ofTrack:assetVideoTrack atTime:CMTimeMake(0, 1) error:nil];
    [videoTrack setPreferredTransform:assetVideoTrack.preferredTransform];
    
    if ([[asset tracksWithMediaType:AVMediaTypeAudio] count]>0)
    {
        AVAssetTrack *assetAudioTrack = [[asset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
        [audioTrack insertTimeRange:assetAudioTrack.timeRange ofTrack:assetAudioTrack atTime:CMTimeMake(0, 1) error:nil];
    }
    else
    {
        NSLog(@"Reminder: video hasn't audio!");
    }
    
    // 4. Effects
    //æ•ˆæœ
    CALayer *parentLayer = [CALayer layer];
    CALayer *videoLayer = [CALayer layer];
    parentLayer.frame = CGRectMake(0, 0, assetVideoTrack.naturalSize.width, assetVideoTrack.naturalSize.height);
    videoLayer.frame = CGRectMake(0, 0, assetVideoTrack.naturalSize.width, assetVideoTrack.naturalSize.height);
    [parentLayer addSublayer:videoLayer];
    
    // Animation effects
    NSMutableArray *animatedLayers = [[NSMutableArray alloc] init];
    //å¯ä»¥ç•™ç€
    NSArray *headArr = [[DLYThemesData sharedInstance] getHeadImageArray];
    NSArray *footArr = [[DLYThemesData sharedInstance] getFootImageArray];
    NSMutableArray *headArray = [NSMutableArray arrayWithArray:headArr];
    NSMutableArray *footArray = [NSMutableArray arrayWithArray:footArr];
    
    CALayer *animatedLayer1 = [self buildAnimationImages:assetVideoTrack.naturalSize imagesArray:headArray withTime:0.1];
    if (animatedLayer1) {
        [animatedLayers addObject:(id)animatedLayer1];
    }
    
    CALayer *animatedLayer2 = [self buildAnimationImages:assetVideoTrack.naturalSize imagesArray:footArray withTime:53.0];
    if (animatedLayer2) {
        [animatedLayers addObject:(id)animatedLayer2];
    }
    
    if (animatedLayers && [animatedLayers count] > 0) {
        for (CALayer *animatedLayer in animatedLayers) {
            [parentLayer addSublayer:animatedLayer];
        }
    }
    
    // Make a "pass through video track" video composition.
    AVMutableVideoCompositionInstruction *passThroughInstruction = [AVMutableVideoCompositionInstruction videoCompositionInstruction];
    passThroughInstruction.timeRange = CMTimeRangeMake(kCMTimeZero, [asset duration]);
    
    AVMutableVideoCompositionLayerInstruction *passThroughLayer = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:assetVideoTrack];
    passThroughInstruction.layerInstructions = [NSArray arrayWithObject:passThroughLayer];
    
    AVMutableVideoComposition *videoComposition = [AVMutableVideoComposition videoComposition];
    videoComposition.instructions = [NSArray arrayWithObject:passThroughInstruction];
    videoComposition.animationTool = [AVVideoCompositionCoreAnimationTool videoCompositionCoreAnimationToolWithPostProcessingAsVideoLayer:videoLayer inLayer:parentLayer];
    videoComposition.frameDuration = CMTimeMake(1, 60); // 30 fps
    videoComposition.renderSize =  assetVideoTrack.naturalSize;
    
    parentLayer = nil;
    if (animatedLayers) {
        [animatedLayers removeAllObjects];
        animatedLayers = nil;
    }
    
    // 5. Music effect
    // 6. Export to mp4 ï¼ˆAttention: iOS 5.0ä¸æ”¯æŒå¯¼å‡ºMP4ï¼Œä¼šcrashï¼‰
    //    NSString *mp4Quality = AVAssetExportPresetMediumQuality; //AVAssetExportPresetPassthrough
    NSString *exportPath = exportVideoFile;
    NSURL *exportUrl = [NSURL fileURLWithPath:[self returnFormatString:exportPath]];
    
    _exportSession = [[AVAssetExportSession alloc] initWithAsset:composition presetName:AVAssetExportPreset1280x720];
    _exportSession.outputURL = exportUrl;
    _exportSession.outputFileType = AVFileTypeMPEG4;
    _exportSession.shouldOptimizeForNetworkUse = YES;
    
    if (videoComposition) {
        _exportSession.videoComposition = videoComposition;
    }
    
    // 7. Success status
    [_exportSession exportAsynchronouslyWithCompletionHandler:^{
        switch ([_exportSession status])
        {
            case AVAssetExportSessionStatusCompleted:
            {
                dispatch_async(dispatch_get_main_queue(), ^{
                    
                    NSLog(@"MP4 Successful!");
                    callBlock(exportUrl,exportPath);
                    
                });
                
                break;
            }
            case AVAssetExportSessionStatusFailed:
            {
                dispatch_async(dispatch_get_main_queue(), ^{
                    
                    // Close timer
                    NSLog(@"å¯¼å‡ºå¤±è´¥");
                    
                });
                
                NSLog(@"Export failed: %@", [[_exportSession error] localizedDescription]);
                
                break;
            }
            case AVAssetExportSessionStatusCancelled:
            {
                NSLog(@"Export canceled");
                break;
            }
            case AVAssetExportSessionStatusWaiting:
            {
                NSLog(@"Export Waiting");
                break;
            }
            case AVAssetExportSessionStatusExporting:
            {
                NSLog(@"Export Exporting");
                break;
            }
            default:
                break;
        }
        
        _exportSession = nil;
        
        if (asset){ }
    }];
    
    return YES;
}
//ç”ŸæˆåŠ¨ç”»
- (CALayer*)buildAnimationImages:(CGSize)viewBounds imagesArray:(NSMutableArray *)imagesArray withTime:(float)beginTime {
    
    if ([imagesArray count] < 1)
    {
        return nil;
    }
    
    // Contains CMTime array for the time duration [0-1]
    NSMutableArray *keyTimesArray = [[NSMutableArray alloc] init];
    double currentTime = CMTimeGetSeconds(kCMTimeZero);
    NSLog(@"currentDuration %f",currentTime);
    
    for (int seed = 0; seed < [imagesArray count]; seed++)
    {
        NSNumber *tempTime = [NSNumber numberWithFloat:(currentTime + (float)seed/[imagesArray count])];
        [keyTimesArray addObject:tempTime];
    }
    
    //    UIImage *image = [UIImage imageWithCGImage:(CGImageRef)imagesArray[0]];
    //    AVSynchronizedLayer *animationLayer = [CALayer layer];
    CALayer *animationLayer = [CALayer layer];
    
    animationLayer.opacity = 1.0;
    animationLayer.frame = CGRectMake(0, 0, 900, 600);
    animationLayer.position = CGPointMake(640, 360);
    
    CAKeyframeAnimation *frameAnimation = [[CAKeyframeAnimation alloc] init];
    frameAnimation.beginTime = beginTime;
    [frameAnimation setKeyPath:@"contents"];
    frameAnimation.calculationMode = kCAAnimationDiscrete;
    //æ³¨é‡Šæ‰å°±OKäº† æ˜¯å¦ç•™ç€æœ€åä¸€å¼ æˆ–æŸä¸€å¼ 
    //    [animationLayer setContents:[imagesArray lastObject]];
    
    frameAnimation.autoreverses = NO;
    frameAnimation.duration = 5.0;
    frameAnimation.repeatCount = 1;
    [frameAnimation setValues:imagesArray];
    [frameAnimation setKeyTimes:keyTimesArray];
    //    [frameAnimation setRemovedOnCompletion:NO];
    [animationLayer addAnimation:frameAnimation forKey:@"contents"];
    //        if (keyTimesArray)
    //        {
    //            [keyTimesArray release];
    //            keyTimesArray = nil;
    //        }
    //
    //        if (frameAnimation)
    //        {
    //            [frameAnimation release];
    //            frameAnimation = nil;
    //        }
    return animationLayer;
}
- (NSString *)returnFormatString:(NSString *)str {
    return [str stringByReplacingOccurrencesOfString:@" " withString:@" "];
}
@end
